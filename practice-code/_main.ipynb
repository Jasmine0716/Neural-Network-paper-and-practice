{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1A0gSeOqQVu",
        "outputId": "222e98f4-5dbd-4c00-f0db-88645854d914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1uS2-NeqS8r",
        "outputId": "5d9a07a4-978f-4ce4-e8a4-f08170910b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"drive/My Drive/MachineLearningPractice/tools\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearningPractice/tools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLbaqR0QqU4m",
        "outputId": "a5206800-0f43-4033-f8b3-b146a8de7cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "!pip install import-ipynb\n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=37050a3adec689d834bd7bbac1c83611eb8391f7cec03e4a49c8f27edb89ffa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH-bqApyqWgf"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MachineLearningPractice')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX8hiFITnLvL"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import import_ipynb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiPhz2kTngyp",
        "outputId": "9a3ac64d-f35d-452e-bb65-d5c9d7d1e813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from models.vgg import *\n",
        "from models.alexnet import *\n",
        "from tools.utils import _initialize_weights\n",
        "from tools.utils import LoadData\n",
        "# import tool.colab_init"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/drive/My Drive/MachineLearningPractice/models/vgg.ipynb\n",
            "importing Jupyter notebook from /content/drive/My Drive/MachineLearningPractice/models/alexnet.ipynb\n",
            "importing Jupyter notebook from /content/drive/My Drive/MachineLearningPractice/tools/utils.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdlG0QyMns-c"
      },
      "source": [
        "class Solver(object):\n",
        "  def __init__(self,config):\n",
        "    self.ModelType = config.model\n",
        "    self.model = None\n",
        "    self.lr = config.lr\n",
        "    self.epochs = config.epoch\n",
        "    self.device = None\n",
        "    self.DataType = \"Cifar10\"\n",
        "    # self.train_batch_size = config.trainBatchSize\n",
        "    # self.test_batch_size = config.testBatchSize\n",
        "    self.criterion = None\n",
        "    self.optimizer = None\n",
        "    self.scheduler = None\n",
        "  \n",
        "  def DataLoading(self):\n",
        "    dataset = LoadData(self.DataType)\n",
        "    self.TrainSet = dataset['trainset']\n",
        "    self.TestSet = dataset['testset']\n",
        "    self.classes = dataset['classes']\n",
        "\n",
        "  def ModelLoading(self):\n",
        "    self.model = eval(self.ModelType)().to(self.device)\n",
        "    _initialize_weights(self.model)\n",
        "    self.optimizer = optim.SGD(self.model.parameters(),lr=self.lr,momentum=0.9,weight_decay=0.0006)\n",
        "    # self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "    #scheduler\n",
        "    self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "    \n",
        "  def train(self,epoch):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_num = 0\n",
        "    \n",
        "    # kbar = pkbar.Kbar(target=len(self.TrainSet), epoch=epoch, num_epochs=self.epochs+1, width=10, always_stateful=False)\n",
        "    for batch_num, (data, target) in enumerate(self.TrainSet):\n",
        "      data, target = data.to(self.device), target.to(self.device)\n",
        "      self.optimizer.zero_grad()\n",
        "      output = self.model(data)\n",
        "      loss = self.criterion(output, target)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      prediction = torch.max(output, 1) #return a tuple\n",
        "      total_num += target.size(0)\n",
        "      total_correct += torch.sum((prediction[1] == target).int()).numpy()\n",
        "\n",
        "      # kbar.update(batch_num,values=[(\"Loss\",total_loss/(batch_num+1)),(\"Current Loss\",loss.item()),(\"Acc\",total_correct/total_num),(\"Correct\",total_correct),(\"Sum\",total_num)])\n",
        "      if (batch_num%100==0):\n",
        "          print(\"epoch:{:d} batch:{:d} loss:{:.4f} acc:{:.4f}({:d}/{:d})\".format(epoch,batch_num,total_loss/(batch_num+1),total_correct/total_num,total_correct,total_num))\n",
        "    return total_loss, total_correct/total_num\n",
        "\n",
        "  def test(self,epoch):\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    test_num = 0\n",
        "\n",
        "    # kbar = pkbar.Kbar(target=len(self.TestSet), epoch=epoch, num_epochs=self.epochs+1, width=10, always_stateful=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_num, (data, target) in enumerate(self.TestSet):\n",
        "        data, target = data.to(self.device), target.to(self.device)\n",
        "        output = self.model(data)\n",
        "        loss = self.criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        prediction = torch.max(output, 1) #return a tuple\n",
        "        test_num += target.size(0)\n",
        "        test_correct += torch.sum((prediction[1] == target).int()).numpy()\n",
        "\n",
        "        # kbar.update(batch_num,values=[(\"Loss\",test_loss/(batch_num+1)),(\"Acc\",test_correct/test_num),(\"Correct\",test_correct),(\"Sum\",test_num)])\n",
        "        if (batch_num%100==0):\n",
        "          print(\"epoch:{:d} batch:{:d} loss:{:.4f} acc:{:.4f}({:d}/{:d})\".format(epoch,batch_num,test_loss/(batch_num+1),test_correct/test_num,test_correct,test_num))\n",
        "    return test_loss, test_correct/test_num\n",
        "\n",
        "  def save(self):\n",
        "    model_path = \"model.pth\"\n",
        "    torch.save(self.model,model_path)\n",
        "    print(\"Saved in {}\".format(model_path))\n",
        "\n",
        "  # load model data??\n",
        "\n",
        "  def run(self):\n",
        "    self.DataLoading()\n",
        "    self.ModelLoading()\n",
        "    accuracy = 0\n",
        "    for epoch in range(1,self.epochs+1):\n",
        "      # scheduler\n",
        "      train_result = self.train(epoch)\n",
        "      print(train_result)\n",
        "      test_result = self.test(epoch)\n",
        "      best_accuracy = max(accuracy,test_result[1])\n",
        "      if epoch == self.epochs:\n",
        "        print(\"\\n==>Best Performance: %.3f%%\"%(accuracy*100))\n",
        "        self.save()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5DmoYytqtWO"
      },
      "source": [
        "def main():\n",
        "  parser = argparse.ArgumentParser(description=\"cifar-10 with PyTorch\")\n",
        "  parser.add_argument('--model',default='VGG19',type=str,help='model type for training')\n",
        "  parser.add_argument('--lr',default=0.005,type=float,help='learning rate')\n",
        "  parser.add_argument('--epoch',default=3,type=int,help=\"epochs for training\")\n",
        "  parser.add_argument('--trainBatchSize',default=100,type=int,help=\"batch size for training\")\n",
        "  parser.add_argument('--testBatchSize',default=100,type=int,help=\"batche size for test\")\n",
        "  parser.add_argument('--cuda',default=torch.cuda.is_available(),type=bool,help='whether cuda is in use')\n",
        "  parser.add_argument('-f')\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  solver = Solver(args)\n",
        "  solver.run()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hblSWWMUqwPP",
        "outputId": "38116f16-2041-4166-dab4-10378295c521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearningPractice/tools/utils.ipynb:4: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/content/drive/My Drive/MachineLearningPractice/tools/utils.ipynb:11: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:1 batch:0 loss:2.3024 acc:0.2500(1/4)\n",
            "epoch:1 batch:100 loss:2.3023 acc:0.1015(41/404)\n",
            "epoch:1 batch:200 loss:2.3045 acc:0.1020(82/804)\n",
            "epoch:1 batch:300 loss:2.3014 acc:0.1013(122/1204)\n",
            "epoch:1 batch:400 loss:2.2950 acc:0.1116(179/1604)\n",
            "epoch:1 batch:500 loss:2.2948 acc:0.1173(235/2004)\n",
            "epoch:1 batch:600 loss:2.2961 acc:0.1119(269/2404)\n",
            "epoch:1 batch:700 loss:2.2977 acc:0.1106(310/2804)\n",
            "epoch:1 batch:800 loss:2.2987 acc:0.1099(352/3204)\n",
            "epoch:1 batch:900 loss:2.2988 acc:0.1107(399/3604)\n",
            "epoch:1 batch:1000 loss:2.2998 acc:0.1106(443/4004)\n",
            "epoch:1 batch:1100 loss:2.3005 acc:0.1104(486/4404)\n",
            "epoch:1 batch:1200 loss:2.3012 acc:0.1087(522/4804)\n",
            "epoch:1 batch:1300 loss:2.3017 acc:0.1078(561/5204)\n",
            "epoch:1 batch:1400 loss:2.3021 acc:0.1074(602/5604)\n",
            "epoch:1 batch:1500 loss:2.3023 acc:0.1073(644/6004)\n",
            "epoch:1 batch:1600 loss:2.3026 acc:0.1056(676/6404)\n",
            "epoch:1 batch:1700 loss:2.3024 acc:0.1051(715/6804)\n",
            "epoch:1 batch:1800 loss:2.3025 acc:0.1051(757/7204)\n",
            "epoch:1 batch:1900 loss:2.3029 acc:0.1027(781/7604)\n",
            "epoch:1 batch:2000 loss:2.3030 acc:0.1024(820/8004)\n",
            "epoch:1 batch:2100 loss:2.3030 acc:0.1034(869/8404)\n",
            "epoch:1 batch:2200 loss:2.3035 acc:0.1027(904/8804)\n",
            "epoch:1 batch:2300 loss:2.3033 acc:0.1030(948/9204)\n",
            "epoch:1 batch:2400 loss:2.3036 acc:0.1027(986/9604)\n",
            "epoch:1 batch:2500 loss:2.3034 acc:0.1030(1030/10004)\n",
            "epoch:1 batch:2600 loss:2.3036 acc:0.1026(1067/10404)\n",
            "epoch:1 batch:2700 loss:2.3037 acc:0.1024(1106/10804)\n",
            "epoch:1 batch:2800 loss:2.3039 acc:0.1024(1147/11204)\n",
            "epoch:1 batch:2900 loss:2.3039 acc:0.1029(1194/11604)\n",
            "epoch:1 batch:3000 loss:2.3039 acc:0.1035(1242/12004)\n",
            "epoch:1 batch:3100 loss:2.3037 acc:0.1036(1285/12404)\n",
            "epoch:1 batch:3200 loss:2.3040 acc:0.1029(1318/12804)\n",
            "epoch:1 batch:3300 loss:2.3041 acc:0.1028(1358/13204)\n",
            "epoch:1 batch:3400 loss:2.3041 acc:0.1029(1400/13604)\n",
            "epoch:1 batch:3500 loss:2.3040 acc:0.1038(1454/14004)\n",
            "epoch:1 batch:3600 loss:2.3041 acc:0.1040(1498/14404)\n",
            "epoch:1 batch:3700 loss:2.3042 acc:0.1038(1536/14804)\n",
            "epoch:1 batch:3800 loss:2.3043 acc:0.1034(1572/15204)\n",
            "epoch:1 batch:3900 loss:2.3043 acc:0.1036(1617/15604)\n",
            "epoch:1 batch:4000 loss:2.3044 acc:0.1035(1657/16004)\n",
            "epoch:1 batch:4100 loss:2.3045 acc:0.1034(1696/16404)\n",
            "epoch:1 batch:4200 loss:2.3044 acc:0.1033(1736/16804)\n",
            "epoch:1 batch:4300 loss:2.3044 acc:0.1031(1773/17204)\n",
            "epoch:1 batch:4400 loss:2.3044 acc:0.1033(1818/17604)\n",
            "epoch:1 batch:4500 loss:2.3044 acc:0.1031(1856/18004)\n",
            "epoch:1 batch:4600 loss:2.3044 acc:0.1032(1900/18404)\n",
            "epoch:1 batch:4700 loss:2.3044 acc:0.1031(1939/18804)\n",
            "epoch:1 batch:4800 loss:2.3045 acc:0.1031(1979/19204)\n",
            "epoch:1 batch:4900 loss:2.3043 acc:0.1035(2029/19604)\n",
            "epoch:1 batch:5000 loss:2.3043 acc:0.1037(2074/20004)\n",
            "epoch:1 batch:5100 loss:2.3044 acc:0.1035(2112/20404)\n",
            "epoch:1 batch:5200 loss:2.3044 acc:0.1037(2158/20804)\n",
            "epoch:1 batch:5300 loss:2.3045 acc:0.1036(2196/21204)\n",
            "epoch:1 batch:5400 loss:2.3045 acc:0.1038(2243/21604)\n",
            "epoch:1 batch:5500 loss:2.3044 acc:0.1039(2287/22004)\n",
            "epoch:1 batch:5600 loss:2.3045 acc:0.1043(2336/22404)\n",
            "epoch:1 batch:5700 loss:2.3045 acc:0.1042(2377/22804)\n",
            "epoch:1 batch:5800 loss:2.3046 acc:0.1042(2417/23204)\n",
            "epoch:1 batch:5900 loss:2.3046 acc:0.1041(2456/23604)\n",
            "epoch:1 batch:6000 loss:2.3046 acc:0.1041(2499/24004)\n",
            "epoch:1 batch:6100 loss:2.3046 acc:0.1040(2539/24404)\n",
            "epoch:1 batch:6200 loss:2.3046 acc:0.1035(2568/24804)\n",
            "epoch:1 batch:6300 loss:2.3046 acc:0.1035(2609/25204)\n",
            "epoch:1 batch:6400 loss:2.3046 acc:0.1033(2645/25604)\n",
            "epoch:1 batch:6500 loss:2.3047 acc:0.1032(2683/26004)\n",
            "epoch:1 batch:6600 loss:2.3046 acc:0.1038(2740/26404)\n",
            "epoch:1 batch:6700 loss:2.3046 acc:0.1038(2781/26804)\n",
            "epoch:1 batch:6800 loss:2.3047 acc:0.1036(2819/27204)\n",
            "epoch:1 batch:6900 loss:2.3047 acc:0.1034(2854/27604)\n",
            "epoch:1 batch:7000 loss:2.3047 acc:0.1032(2890/28004)\n",
            "epoch:1 batch:7100 loss:2.3047 acc:0.1030(2927/28404)\n",
            "epoch:1 batch:7200 loss:2.3047 acc:0.1030(2966/28804)\n",
            "epoch:1 batch:7300 loss:2.3047 acc:0.1032(3014/29204)\n",
            "epoch:1 batch:7400 loss:2.3046 acc:0.1034(3060/29604)\n",
            "epoch:1 batch:7500 loss:2.3046 acc:0.1031(3094/30004)\n",
            "epoch:1 batch:7600 loss:2.3046 acc:0.1032(3138/30404)\n",
            "epoch:1 batch:7700 loss:2.3045 acc:0.1034(3186/30804)\n",
            "epoch:1 batch:7800 loss:2.3045 acc:0.1037(3236/31204)\n",
            "epoch:1 batch:7900 loss:2.3045 acc:0.1037(3278/31604)\n",
            "epoch:1 batch:8000 loss:2.3046 acc:0.1036(3315/32004)\n",
            "epoch:1 batch:8100 loss:2.3046 acc:0.1034(3349/32404)\n",
            "epoch:1 batch:8200 loss:2.3046 acc:0.1034(3391/32804)\n",
            "epoch:1 batch:8300 loss:2.3046 acc:0.1034(3434/33204)\n",
            "epoch:1 batch:8400 loss:2.3046 acc:0.1032(3467/33604)\n",
            "epoch:1 batch:8500 loss:2.3046 acc:0.1031(3507/34004)\n",
            "epoch:1 batch:8600 loss:2.3046 acc:0.1029(3541/34404)\n",
            "epoch:1 batch:8700 loss:2.3046 acc:0.1027(3575/34804)\n",
            "epoch:1 batch:8800 loss:2.3046 acc:0.1029(3624/35204)\n",
            "epoch:1 batch:8900 loss:2.3047 acc:0.1028(3660/35604)\n",
            "epoch:1 batch:9000 loss:2.3047 acc:0.1025(3692/36004)\n",
            "epoch:1 batch:9100 loss:2.3046 acc:0.1025(3730/36404)\n",
            "epoch:1 batch:9200 loss:2.3047 acc:0.1022(3762/36804)\n",
            "epoch:1 batch:9300 loss:2.3047 acc:0.1022(3803/37204)\n",
            "epoch:1 batch:9400 loss:2.3047 acc:0.1024(3849/37604)\n",
            "epoch:1 batch:9500 loss:2.3048 acc:0.1021(3881/38004)\n",
            "epoch:1 batch:9600 loss:2.3048 acc:0.1021(3920/38404)\n",
            "epoch:1 batch:9700 loss:2.3048 acc:0.1018(3952/38804)\n",
            "epoch:1 batch:9800 loss:2.3048 acc:0.1017(3986/39204)\n",
            "epoch:1 batch:9900 loss:2.3048 acc:0.1015(4019/39604)\n",
            "epoch:1 batch:10000 loss:2.3048 acc:0.1015(4061/40004)\n",
            "epoch:1 batch:10100 loss:2.3049 acc:0.1015(4099/40404)\n",
            "epoch:1 batch:10200 loss:2.3048 acc:0.1014(4137/40804)\n",
            "epoch:1 batch:10300 loss:2.3048 acc:0.1013(4174/41204)\n",
            "epoch:1 batch:10400 loss:2.3048 acc:0.1012(4211/41604)\n",
            "epoch:1 batch:10500 loss:2.3048 acc:0.1012(4251/42004)\n",
            "epoch:1 batch:10600 loss:2.3049 acc:0.1011(4285/42404)\n",
            "epoch:1 batch:10700 loss:2.3048 acc:0.1010(4322/42804)\n",
            "epoch:1 batch:10800 loss:2.3048 acc:0.1008(4357/43204)\n",
            "epoch:1 batch:10900 loss:2.3049 acc:0.1010(4402/43604)\n",
            "epoch:1 batch:11000 loss:2.3049 acc:0.1011(4447/44004)\n",
            "epoch:1 batch:11100 loss:2.3049 acc:0.1009(4480/44404)\n",
            "epoch:1 batch:11200 loss:2.3049 acc:0.1010(4523/44804)\n",
            "epoch:1 batch:11300 loss:2.3049 acc:0.1007(4552/45204)\n",
            "epoch:1 batch:11400 loss:2.3049 acc:0.1008(4599/45604)\n",
            "epoch:1 batch:11500 loss:2.3049 acc:0.1007(4631/46004)\n",
            "epoch:1 batch:11600 loss:2.3049 acc:0.1006(4667/46404)\n",
            "epoch:1 batch:11700 loss:2.3049 acc:0.1005(4705/46804)\n",
            "epoch:1 batch:11800 loss:2.3049 acc:0.1005(4744/47204)\n",
            "epoch:1 batch:11900 loss:2.3049 acc:0.1005(4784/47604)\n",
            "epoch:1 batch:12000 loss:2.3050 acc:0.1003(4817/48004)\n",
            "epoch:1 batch:12100 loss:2.3050 acc:0.1005(4863/48404)\n",
            "epoch:1 batch:12200 loss:2.3050 acc:0.1004(4898/48804)\n",
            "epoch:1 batch:12300 loss:2.3050 acc:0.1003(4937/49204)\n",
            "epoch:1 batch:12400 loss:2.3050 acc:0.1004(4980/49604)\n",
            "(28812.081960320473, 0.10042)\n",
            "epoch:1 batch:0 loss:2.2209 acc:0.5000(2/4)\n",
            "epoch:1 batch:100 loss:2.3013 acc:0.1139(46/404)\n",
            "epoch:1 batch:200 loss:2.3050 acc:0.1107(89/804)\n",
            "epoch:1 batch:300 loss:2.3042 acc:0.1063(128/1204)\n",
            "epoch:1 batch:400 loss:2.3058 acc:0.1047(168/1604)\n",
            "epoch:1 batch:500 loss:2.3068 acc:0.1023(205/2004)\n",
            "epoch:1 batch:600 loss:2.3068 acc:0.1011(243/2404)\n",
            "epoch:1 batch:700 loss:2.3070 acc:0.1002(281/2804)\n",
            "epoch:1 batch:800 loss:2.3065 acc:0.1011(324/3204)\n",
            "epoch:1 batch:900 loss:2.3068 acc:0.1004(362/3604)\n",
            "epoch:1 batch:1000 loss:2.3065 acc:0.1031(413/4004)\n",
            "epoch:1 batch:1100 loss:2.3068 acc:0.1008(444/4404)\n",
            "epoch:1 batch:1200 loss:2.3067 acc:0.1014(487/4804)\n",
            "epoch:1 batch:1300 loss:2.3066 acc:0.1009(525/5204)\n",
            "epoch:1 batch:1400 loss:2.3062 acc:0.1026(575/5604)\n",
            "epoch:1 batch:1500 loss:2.3065 acc:0.1023(614/6004)\n",
            "epoch:1 batch:1600 loss:2.3064 acc:0.1027(658/6404)\n",
            "epoch:1 batch:1700 loss:2.3063 acc:0.1014(690/6804)\n",
            "epoch:1 batch:1800 loss:2.3066 acc:0.0999(720/7204)\n",
            "epoch:1 batch:1900 loss:2.3070 acc:0.0992(754/7604)\n",
            "epoch:1 batch:2000 loss:2.3066 acc:0.0991(793/8004)\n",
            "epoch:1 batch:2100 loss:2.3071 acc:0.0983(826/8404)\n",
            "epoch:1 batch:2200 loss:2.3070 acc:0.0989(871/8804)\n",
            "epoch:1 batch:2300 loss:2.3071 acc:0.0989(910/9204)\n",
            "epoch:1 batch:2400 loss:2.3072 acc:0.0984(945/9604)\n",
            "epoch:1 batch:2500 loss:2.3073 acc:0.0981(981/10004)\n",
            "epoch:1 batch:2600 loss:2.3073 acc:0.0981(1021/10404)\n",
            "epoch:1 batch:2700 loss:2.3075 acc:0.0973(1051/10804)\n",
            "epoch:1 batch:2800 loss:2.3077 acc:0.0968(1085/11204)\n",
            "epoch:1 batch:2900 loss:2.3075 acc:0.0970(1126/11604)\n",
            "epoch:1 batch:3000 loss:2.3073 acc:0.0974(1169/12004)\n",
            "epoch:1 batch:3100 loss:2.3075 acc:0.0968(1201/12404)\n",
            "epoch:1 batch:3200 loss:2.3073 acc:0.0969(1241/12804)\n",
            "epoch:1 batch:3300 loss:2.3073 acc:0.0969(1279/13204)\n",
            "epoch:1 batch:3400 loss:2.3072 acc:0.0970(1319/13604)\n",
            "epoch:1 batch:3500 loss:2.3073 acc:0.0972(1361/14004)\n",
            "epoch:1 batch:3600 loss:2.3073 acc:0.0971(1399/14404)\n",
            "epoch:1 batch:3700 loss:2.3073 acc:0.0973(1441/14804)\n",
            "epoch:1 batch:3800 loss:2.3071 acc:0.0976(1484/15204)\n",
            "epoch:1 batch:3900 loss:2.3070 acc:0.0974(1520/15604)\n",
            "epoch:1 batch:4000 loss:2.3072 acc:0.0971(1554/16004)\n",
            "epoch:1 batch:4100 loss:2.3072 acc:0.0972(1595/16404)\n",
            "epoch:1 batch:4200 loss:2.3072 acc:0.0977(1642/16804)\n",
            "epoch:1 batch:4300 loss:2.3072 acc:0.0977(1681/17204)\n",
            "epoch:1 batch:4400 loss:2.3071 acc:0.0980(1725/17604)\n",
            "epoch:1 batch:4500 loss:2.3069 acc:0.0983(1770/18004)\n",
            "epoch:1 batch:4600 loss:2.3071 acc:0.0979(1802/18404)\n",
            "epoch:1 batch:4700 loss:2.3071 acc:0.0979(1840/18804)\n",
            "epoch:1 batch:4800 loss:2.3071 acc:0.0979(1881/19204)\n",
            "epoch:1 batch:4900 loss:2.3070 acc:0.0982(1925/19604)\n",
            "epoch:1 batch:5000 loss:2.3070 acc:0.0983(1966/20004)\n",
            "epoch:1 batch:5100 loss:2.3071 acc:0.0979(1997/20404)\n",
            "epoch:1 batch:5200 loss:2.3072 acc:0.0980(2038/20804)\n",
            "epoch:1 batch:5300 loss:2.3072 acc:0.0978(2074/21204)\n",
            "epoch:1 batch:5400 loss:2.3072 acc:0.0985(2129/21604)\n",
            "epoch:1 batch:5500 loss:2.3072 acc:0.0988(2173/22004)\n",
            "epoch:1 batch:5600 loss:2.3070 acc:0.0990(2218/22404)\n",
            "epoch:1 batch:5700 loss:2.3071 acc:0.0987(2251/22804)\n",
            "epoch:1 batch:5800 loss:2.3071 acc:0.0985(2285/23204)\n",
            "epoch:1 batch:5900 loss:2.3071 acc:0.0986(2328/23604)\n",
            "epoch:1 batch:6000 loss:2.3070 acc:0.0991(2378/24004)\n",
            "epoch:1 batch:6100 loss:2.3070 acc:0.0995(2429/24404)\n",
            "epoch:1 batch:6200 loss:2.3070 acc:0.0995(2469/24804)\n",
            "epoch:1 batch:6300 loss:2.3070 acc:0.0990(2495/25204)\n",
            "epoch:1 batch:6400 loss:2.3070 acc:0.0987(2527/25604)\n",
            "epoch:1 batch:6500 loss:2.3070 acc:0.0985(2561/26004)\n",
            "epoch:1 batch:6600 loss:2.3070 acc:0.0985(2600/26404)\n",
            "epoch:1 batch:6700 loss:2.3069 acc:0.0989(2651/26804)\n",
            "epoch:1 batch:6800 loss:2.3068 acc:0.0994(2703/27204)\n",
            "epoch:1 batch:6900 loss:2.3068 acc:0.0995(2747/27604)\n",
            "epoch:1 batch:7000 loss:2.3068 acc:0.0997(2792/28004)\n",
            "epoch:1 batch:7100 loss:2.3068 acc:0.0997(2831/28404)\n",
            "epoch:1 batch:7200 loss:2.3067 acc:0.0997(2871/28804)\n",
            "epoch:1 batch:7300 loss:2.3067 acc:0.1001(2922/29204)\n",
            "epoch:1 batch:7400 loss:2.3067 acc:0.1001(2962/29604)\n",
            "epoch:1 batch:7500 loss:2.3067 acc:0.0998(2995/30004)\n",
            "epoch:1 batch:7600 loss:2.3068 acc:0.0997(3031/30404)\n",
            "epoch:1 batch:7700 loss:2.3068 acc:0.0999(3076/30804)\n",
            "epoch:1 batch:7800 loss:2.3069 acc:0.0998(3115/31204)\n",
            "epoch:1 batch:7900 loss:2.3069 acc:0.0996(3148/31604)\n",
            "epoch:1 batch:8000 loss:2.3068 acc:0.0999(3196/32004)\n",
            "epoch:1 batch:8100 loss:2.3069 acc:0.0998(3235/32404)\n",
            "epoch:1 batch:8200 loss:2.3069 acc:0.0997(3270/32804)\n",
            "epoch:1 batch:8300 loss:2.3070 acc:0.0995(3304/33204)\n",
            "epoch:1 batch:8400 loss:2.3070 acc:0.0993(3336/33604)\n",
            "epoch:1 batch:8500 loss:2.3070 acc:0.0995(3383/34004)\n",
            "epoch:1 batch:8600 loss:2.3070 acc:0.0994(3419/34404)\n",
            "epoch:1 batch:8700 loss:2.3069 acc:0.0995(3464/34804)\n",
            "epoch:1 batch:8800 loss:2.3069 acc:0.0996(3507/35204)\n",
            "epoch:1 batch:8900 loss:2.3069 acc:0.0996(3545/35604)\n",
            "epoch:1 batch:9000 loss:2.3069 acc:0.0995(3583/36004)\n",
            "epoch:1 batch:9100 loss:2.3069 acc:0.0995(3621/36404)\n",
            "epoch:1 batch:9200 loss:2.3069 acc:0.0996(3664/36804)\n",
            "epoch:1 batch:9300 loss:2.3069 acc:0.0997(3711/37204)\n",
            "epoch:1 batch:9400 loss:2.3068 acc:0.0999(3756/37604)\n",
            "epoch:1 batch:9500 loss:2.3068 acc:0.0995(3783/38004)\n",
            "epoch:1 batch:9600 loss:2.3068 acc:0.0994(3818/38404)\n",
            "epoch:1 batch:9700 loss:2.3069 acc:0.0993(3852/38804)\n",
            "epoch:1 batch:9800 loss:2.3069 acc:0.0991(3887/39204)\n",
            "epoch:1 batch:9900 loss:2.3070 acc:0.0991(3926/39604)\n",
            "epoch:1 batch:10000 loss:2.3069 acc:0.0994(3978/40004)\n",
            "epoch:1 batch:10100 loss:2.3069 acc:0.0997(4029/40404)\n",
            "epoch:1 batch:10200 loss:2.3069 acc:0.0997(4068/40804)\n",
            "epoch:1 batch:10300 loss:2.3069 acc:0.0996(4104/41204)\n",
            "epoch:1 batch:10400 loss:2.3069 acc:0.0998(4153/41604)\n",
            "epoch:1 batch:10500 loss:2.3070 acc:0.0997(4188/42004)\n",
            "epoch:1 batch:10600 loss:2.3070 acc:0.0998(4231/42404)\n",
            "epoch:1 batch:10700 loss:2.3070 acc:0.0997(4268/42804)\n",
            "epoch:1 batch:10800 loss:2.3070 acc:0.0997(4306/43204)\n",
            "epoch:1 batch:10900 loss:2.3069 acc:0.0996(4343/43604)\n",
            "epoch:1 batch:11000 loss:2.3069 acc:0.0997(4385/44004)\n",
            "epoch:1 batch:11100 loss:2.3069 acc:0.0996(4423/44404)\n",
            "epoch:1 batch:11200 loss:2.3069 acc:0.0996(4463/44804)\n",
            "epoch:1 batch:11300 loss:2.3069 acc:0.0999(4515/45204)\n",
            "epoch:1 batch:11400 loss:2.3069 acc:0.0998(4549/45604)\n",
            "epoch:1 batch:11500 loss:2.3069 acc:0.0999(4594/46004)\n",
            "epoch:1 batch:11600 loss:2.3069 acc:0.0999(4634/46404)\n",
            "epoch:1 batch:11700 loss:2.3070 acc:0.0998(4670/46804)\n",
            "epoch:1 batch:11800 loss:2.3069 acc:0.0999(4715/47204)\n",
            "epoch:1 batch:11900 loss:2.3069 acc:0.0999(4754/47604)\n",
            "epoch:1 batch:12000 loss:2.3070 acc:0.0998(4790/48004)\n",
            "epoch:1 batch:12100 loss:2.3070 acc:0.0999(4834/48404)\n",
            "epoch:1 batch:12200 loss:2.3070 acc:0.1000(4878/48804)\n",
            "epoch:1 batch:12300 loss:2.3070 acc:0.1000(4921/49204)\n",
            "epoch:1 batch:12400 loss:2.3070 acc:0.1000(4961/49604)\n",
            "epoch:2 batch:0 loss:2.3180 acc:0.2500(1/4)\n",
            "epoch:2 batch:100 loss:2.3081 acc:0.1188(48/404)\n",
            "epoch:2 batch:200 loss:2.3067 acc:0.1045(84/804)\n",
            "epoch:2 batch:300 loss:2.3050 acc:0.1055(127/1204)\n",
            "epoch:2 batch:400 loss:2.3062 acc:0.0985(158/1604)\n",
            "epoch:2 batch:500 loss:2.3066 acc:0.0973(195/2004)\n",
            "epoch:2 batch:600 loss:2.3068 acc:0.0940(226/2404)\n",
            "epoch:2 batch:700 loss:2.3061 acc:0.0952(267/2804)\n",
            "epoch:2 batch:800 loss:2.3063 acc:0.0952(305/3204)\n",
            "epoch:2 batch:900 loss:2.3062 acc:0.0907(327/3604)\n",
            "epoch:2 batch:1000 loss:2.3061 acc:0.0922(369/4004)\n",
            "epoch:2 batch:1100 loss:2.3055 acc:0.0954(420/4404)\n",
            "epoch:2 batch:1200 loss:2.3057 acc:0.0947(455/4804)\n",
            "epoch:2 batch:1300 loss:2.3058 acc:0.0951(495/5204)\n",
            "epoch:2 batch:1400 loss:2.3060 acc:0.0939(526/5604)\n",
            "epoch:2 batch:1500 loss:2.3059 acc:0.0936(562/6004)\n",
            "epoch:2 batch:1600 loss:2.3057 acc:0.0948(607/6404)\n",
            "epoch:2 batch:1700 loss:2.3057 acc:0.0949(646/6804)\n",
            "epoch:2 batch:1800 loss:2.3058 acc:0.0959(691/7204)\n",
            "epoch:2 batch:1900 loss:2.3059 acc:0.0948(721/7604)\n",
            "epoch:2 batch:2000 loss:2.3059 acc:0.0953(763/8004)\n",
            "epoch:2 batch:2100 loss:2.3056 acc:0.0957(804/8404)\n",
            "epoch:2 batch:2200 loss:2.3058 acc:0.0956(842/8804)\n",
            "epoch:2 batch:2300 loss:2.3057 acc:0.0955(879/9204)\n",
            "epoch:2 batch:2400 loss:2.3057 acc:0.0954(916/9604)\n",
            "epoch:2 batch:2500 loss:2.3058 acc:0.0954(954/10004)\n",
            "epoch:2 batch:2600 loss:2.3059 acc:0.0953(992/10404)\n",
            "epoch:2 batch:2700 loss:2.3060 acc:0.0943(1019/10804)\n",
            "epoch:2 batch:2800 loss:2.3059 acc:0.0951(1065/11204)\n",
            "epoch:2 batch:2900 loss:2.3057 acc:0.0957(1111/11604)\n",
            "epoch:2 batch:3000 loss:2.3059 acc:0.0952(1143/12004)\n",
            "epoch:2 batch:3100 loss:2.3059 acc:0.0956(1186/12404)\n",
            "epoch:2 batch:3200 loss:2.3059 acc:0.0961(1230/12804)\n",
            "epoch:2 batch:3300 loss:2.3059 acc:0.0954(1260/13204)\n",
            "epoch:2 batch:3400 loss:2.3059 acc:0.0949(1291/13604)\n",
            "epoch:2 batch:3500 loss:2.3058 acc:0.0949(1329/14004)\n",
            "epoch:2 batch:3600 loss:2.3058 acc:0.0948(1366/14404)\n",
            "epoch:2 batch:3700 loss:2.3059 acc:0.0947(1402/14804)\n",
            "epoch:2 batch:3800 loss:2.3058 acc:0.0952(1447/15204)\n",
            "epoch:2 batch:3900 loss:2.3057 acc:0.0953(1487/15604)\n",
            "epoch:2 batch:4000 loss:2.3056 acc:0.0953(1525/16004)\n",
            "epoch:2 batch:4100 loss:2.3058 acc:0.0952(1561/16404)\n",
            "epoch:2 batch:4200 loss:2.3058 acc:0.0956(1606/16804)\n",
            "epoch:2 batch:4300 loss:2.3058 acc:0.0954(1642/17204)\n",
            "epoch:2 batch:4400 loss:2.3058 acc:0.0951(1675/17604)\n",
            "epoch:2 batch:4500 loss:2.3057 acc:0.0955(1720/18004)\n",
            "epoch:2 batch:4600 loss:2.3058 acc:0.0953(1754/18404)\n",
            "epoch:2 batch:4700 loss:2.3058 acc:0.0955(1796/18804)\n",
            "epoch:2 batch:4800 loss:2.3058 acc:0.0957(1837/19204)\n",
            "epoch:2 batch:4900 loss:2.3057 acc:0.0964(1889/19604)\n",
            "epoch:2 batch:5000 loss:2.3056 acc:0.0964(1928/20004)\n",
            "epoch:2 batch:5100 loss:2.3057 acc:0.0967(1973/20404)\n",
            "epoch:2 batch:5200 loss:2.3057 acc:0.0968(2014/20804)\n",
            "epoch:2 batch:5300 loss:2.3057 acc:0.0964(2045/21204)\n",
            "epoch:2 batch:5400 loss:2.3057 acc:0.0961(2076/21604)\n",
            "epoch:2 batch:5500 loss:2.3057 acc:0.0965(2124/22004)\n",
            "epoch:2 batch:5600 loss:2.3057 acc:0.0965(2162/22404)\n",
            "epoch:2 batch:5700 loss:2.3057 acc:0.0966(2202/22804)\n",
            "epoch:2 batch:5800 loss:2.3057 acc:0.0964(2238/23204)\n",
            "epoch:2 batch:5900 loss:2.3058 acc:0.0960(2265/23604)\n",
            "epoch:2 batch:6000 loss:2.3058 acc:0.0959(2303/24004)\n",
            "epoch:2 batch:6100 loss:2.3058 acc:0.0960(2344/24404)\n",
            "epoch:2 batch:6200 loss:2.3058 acc:0.0962(2386/24804)\n",
            "epoch:2 batch:6300 loss:2.3058 acc:0.0967(2438/25204)\n",
            "epoch:2 batch:6400 loss:2.3058 acc:0.0966(2474/25604)\n",
            "epoch:2 batch:6500 loss:2.3058 acc:0.0964(2507/26004)\n",
            "epoch:2 batch:6600 loss:2.3057 acc:0.0965(2549/26404)\n",
            "epoch:2 batch:6700 loss:2.3057 acc:0.0973(2609/26804)\n",
            "epoch:2 batch:6800 loss:2.3058 acc:0.0973(2646/27204)\n",
            "epoch:2 batch:6900 loss:2.3057 acc:0.0977(2697/27604)\n",
            "epoch:2 batch:7000 loss:2.3057 acc:0.0974(2727/28004)\n",
            "epoch:2 batch:7100 loss:2.3057 acc:0.0975(2768/28404)\n",
            "epoch:2 batch:7200 loss:2.3057 acc:0.0976(2810/28804)\n",
            "epoch:2 batch:7300 loss:2.3058 acc:0.0975(2847/29204)\n",
            "epoch:2 batch:7400 loss:2.3058 acc:0.0971(2876/29604)\n",
            "epoch:2 batch:7500 loss:2.3058 acc:0.0972(2915/30004)\n",
            "epoch:2 batch:7600 loss:2.3057 acc:0.0973(2957/30404)\n",
            "epoch:2 batch:7700 loss:2.3057 acc:0.0976(3005/30804)\n",
            "epoch:2 batch:7800 loss:2.3057 acc:0.0974(3038/31204)\n",
            "epoch:2 batch:7900 loss:2.3057 acc:0.0972(3072/31604)\n",
            "epoch:2 batch:8000 loss:2.3057 acc:0.0976(3122/32004)\n",
            "epoch:2 batch:8100 loss:2.3057 acc:0.0976(3164/32404)\n",
            "epoch:2 batch:8200 loss:2.3057 acc:0.0976(3203/32804)\n",
            "epoch:2 batch:8300 loss:2.3056 acc:0.0978(3246/33204)\n",
            "epoch:2 batch:8400 loss:2.3055 acc:0.0983(3302/33604)\n",
            "epoch:2 batch:8500 loss:2.3056 acc:0.0983(3342/34004)\n",
            "epoch:2 batch:8600 loss:2.3056 acc:0.0983(3382/34404)\n",
            "epoch:2 batch:8700 loss:2.3056 acc:0.0983(3421/34804)\n",
            "epoch:2 batch:8800 loss:2.3056 acc:0.0982(3456/35204)\n",
            "epoch:2 batch:8900 loss:2.3056 acc:0.0981(3494/35604)\n",
            "epoch:2 batch:9000 loss:2.3056 acc:0.0983(3539/36004)\n",
            "epoch:2 batch:9100 loss:2.3056 acc:0.0985(3586/36404)\n",
            "epoch:2 batch:9200 loss:2.3056 acc:0.0982(3615/36804)\n",
            "epoch:2 batch:9300 loss:2.3056 acc:0.0983(3659/37204)\n",
            "epoch:2 batch:9400 loss:2.3055 acc:0.0987(3710/37604)\n",
            "epoch:2 batch:9500 loss:2.3056 acc:0.0985(3744/38004)\n",
            "epoch:2 batch:9600 loss:2.3056 acc:0.0986(3786/38404)\n",
            "epoch:2 batch:9700 loss:2.3056 acc:0.0990(3840/38804)\n",
            "epoch:2 batch:9800 loss:2.3056 acc:0.0990(3880/39204)\n",
            "epoch:2 batch:9900 loss:2.3056 acc:0.0988(3913/39604)\n",
            "epoch:2 batch:10000 loss:2.3055 acc:0.0989(3956/40004)\n",
            "epoch:2 batch:10100 loss:2.3056 acc:0.0989(3997/40404)\n",
            "epoch:2 batch:10200 loss:2.3056 acc:0.0990(4038/40804)\n",
            "epoch:2 batch:10300 loss:2.3056 acc:0.0989(4075/41204)\n",
            "epoch:2 batch:10400 loss:2.3056 acc:0.0990(4120/41604)\n",
            "epoch:2 batch:10500 loss:2.3056 acc:0.0990(4159/42004)\n",
            "epoch:2 batch:10600 loss:2.3056 acc:0.0991(4202/42404)\n",
            "epoch:2 batch:10700 loss:2.3056 acc:0.0991(4240/42804)\n",
            "epoch:2 batch:10800 loss:2.3056 acc:0.0990(4276/43204)\n",
            "epoch:2 batch:10900 loss:2.3056 acc:0.0991(4320/43604)\n",
            "epoch:2 batch:11000 loss:2.3056 acc:0.0992(4364/44004)\n",
            "epoch:2 batch:11100 loss:2.3055 acc:0.0992(4405/44404)\n",
            "epoch:2 batch:11200 loss:2.3056 acc:0.0992(4443/44804)\n",
            "epoch:2 batch:11300 loss:2.3056 acc:0.0990(4476/45204)\n",
            "epoch:2 batch:11400 loss:2.3056 acc:0.0991(4520/45604)\n",
            "epoch:2 batch:11500 loss:2.3056 acc:0.0991(4558/46004)\n",
            "epoch:2 batch:11600 loss:2.3056 acc:0.0991(4600/46404)\n",
            "epoch:2 batch:11700 loss:2.3056 acc:0.0993(4647/46804)\n",
            "epoch:2 batch:11800 loss:2.3056 acc:0.0990(4674/47204)\n",
            "epoch:2 batch:11900 loss:2.3056 acc:0.0989(4709/47604)\n",
            "epoch:2 batch:12000 loss:2.3056 acc:0.0988(4742/48004)\n",
            "epoch:2 batch:12100 loss:2.3056 acc:0.0987(4777/48404)\n",
            "epoch:2 batch:12200 loss:2.3057 acc:0.0986(4811/48804)\n",
            "epoch:2 batch:12300 loss:2.3056 acc:0.0987(4855/49204)\n",
            "epoch:2 batch:12400 loss:2.3056 acc:0.0990(4909/49604)\n",
            "(28819.524451971054, 0.0993)\n",
            "epoch:2 batch:0 loss:2.2776 acc:0.2500(1/4)\n",
            "epoch:2 batch:100 loss:2.3068 acc:0.0941(38/404)\n",
            "epoch:2 batch:200 loss:2.3089 acc:0.0983(79/804)\n",
            "epoch:2 batch:300 loss:2.3103 acc:0.0930(112/1204)\n",
            "epoch:2 batch:400 loss:2.3081 acc:0.1004(161/1604)\n",
            "epoch:2 batch:500 loss:2.3069 acc:0.1068(214/2004)\n",
            "epoch:2 batch:600 loss:2.3077 acc:0.1044(251/2404)\n",
            "epoch:2 batch:700 loss:2.3086 acc:0.1020(286/2804)\n",
            "epoch:2 batch:800 loss:2.3087 acc:0.1024(328/3204)\n",
            "epoch:2 batch:900 loss:2.3084 acc:0.1038(374/3604)\n",
            "epoch:2 batch:1000 loss:2.3081 acc:0.1054(422/4004)\n",
            "epoch:2 batch:1100 loss:2.3074 acc:0.1067(470/4404)\n",
            "epoch:2 batch:1200 loss:2.3079 acc:0.1045(502/4804)\n",
            "epoch:2 batch:1300 loss:2.3083 acc:0.1030(536/5204)\n",
            "epoch:2 batch:1400 loss:2.3088 acc:0.1019(571/5604)\n",
            "epoch:2 batch:1500 loss:2.3090 acc:0.1018(611/6004)\n",
            "epoch:2 batch:1600 loss:2.3085 acc:0.1029(659/6404)\n",
            "epoch:2 batch:1700 loss:2.3084 acc:0.1024(697/6804)\n",
            "epoch:2 batch:1800 loss:2.3087 acc:0.1023(737/7204)\n",
            "epoch:2 batch:1900 loss:2.3085 acc:0.1015(772/7604)\n",
            "epoch:2 batch:2000 loss:2.3086 acc:0.1007(806/8004)\n",
            "epoch:2 batch:2100 loss:2.3087 acc:0.1001(841/8404)\n",
            "epoch:2 batch:2200 loss:2.3090 acc:0.0997(878/8804)\n",
            "epoch:2 batch:2300 loss:2.3090 acc:0.0994(915/9204)\n",
            "epoch:2 batch:2400 loss:2.3090 acc:0.0992(953/9604)\n",
            "epoch:2 batch:2500 loss:2.3087 acc:0.0999(999/10004)\n",
            "epoch:2 batch:2600 loss:2.3086 acc:0.1003(1044/10404)\n",
            "epoch:2 batch:2700 loss:2.3086 acc:0.1005(1086/10804)\n",
            "epoch:2 batch:2800 loss:2.3087 acc:0.1004(1125/11204)\n",
            "epoch:2 batch:2900 loss:2.3089 acc:0.0996(1156/11604)\n",
            "epoch:2 batch:3000 loss:2.3088 acc:0.0994(1193/12004)\n",
            "epoch:2 batch:3100 loss:2.3087 acc:0.0993(1232/12404)\n",
            "epoch:2 batch:3200 loss:2.3089 acc:0.0990(1267/12804)\n",
            "epoch:2 batch:3300 loss:2.3091 acc:0.0982(1297/13204)\n",
            "epoch:2 batch:3400 loss:2.3093 acc:0.0977(1329/13604)\n",
            "epoch:2 batch:3500 loss:2.3091 acc:0.0984(1378/14004)\n",
            "epoch:2 batch:3600 loss:2.3088 acc:0.0991(1427/14404)\n",
            "epoch:2 batch:3700 loss:2.3086 acc:0.0998(1477/14804)\n",
            "epoch:2 batch:3800 loss:2.3084 acc:0.1007(1531/15204)\n",
            "epoch:2 batch:3900 loss:2.3083 acc:0.1007(1571/15604)\n",
            "epoch:2 batch:4000 loss:2.3084 acc:0.1006(1610/16004)\n",
            "epoch:2 batch:4100 loss:2.3084 acc:0.1005(1649/16404)\n",
            "epoch:2 batch:4200 loss:2.3084 acc:0.1002(1684/16804)\n",
            "epoch:2 batch:4300 loss:2.3082 acc:0.1006(1731/17204)\n",
            "epoch:2 batch:4400 loss:2.3084 acc:0.1000(1761/17604)\n",
            "epoch:2 batch:4500 loss:2.3085 acc:0.0999(1798/18004)\n",
            "epoch:2 batch:4600 loss:2.3085 acc:0.1002(1845/18404)\n",
            "epoch:2 batch:4700 loss:2.3085 acc:0.1004(1887/18804)\n",
            "epoch:2 batch:4800 loss:2.3086 acc:0.1003(1927/19204)\n",
            "epoch:2 batch:4900 loss:2.3086 acc:0.1004(1969/19604)\n",
            "epoch:2 batch:5000 loss:2.3086 acc:0.1004(2009/20004)\n",
            "epoch:2 batch:5100 loss:2.3085 acc:0.1005(2051/20404)\n",
            "epoch:2 batch:5200 loss:2.3085 acc:0.1004(2089/20804)\n",
            "epoch:2 batch:5300 loss:2.3084 acc:0.1005(2132/21204)\n",
            "epoch:2 batch:5400 loss:2.3085 acc:0.1004(2169/21604)\n",
            "epoch:2 batch:5500 loss:2.3085 acc:0.1004(2210/22004)\n",
            "epoch:2 batch:5600 loss:2.3084 acc:0.1006(2254/22404)\n",
            "epoch:2 batch:5700 loss:2.3084 acc:0.1006(2294/22804)\n",
            "epoch:2 batch:5800 loss:2.3084 acc:0.1004(2330/23204)\n",
            "epoch:2 batch:5900 loss:2.3085 acc:0.1003(2368/23604)\n",
            "epoch:2 batch:6000 loss:2.3086 acc:0.1001(2404/24004)\n",
            "epoch:2 batch:6100 loss:2.3087 acc:0.0996(2431/24404)\n",
            "epoch:2 batch:6200 loss:2.3087 acc:0.0994(2466/24804)\n",
            "epoch:2 batch:6300 loss:2.3088 acc:0.0992(2501/25204)\n",
            "epoch:2 batch:6400 loss:2.3087 acc:0.0993(2543/25604)\n",
            "epoch:2 batch:6500 loss:2.3087 acc:0.0994(2584/26004)\n",
            "epoch:2 batch:6600 loss:2.3088 acc:0.0989(2612/26404)\n",
            "epoch:2 batch:6700 loss:2.3090 acc:0.0985(2641/26804)\n",
            "epoch:2 batch:6800 loss:2.3088 acc:0.0991(2697/27204)\n",
            "epoch:2 batch:6900 loss:2.3086 acc:0.0998(2755/27604)\n",
            "epoch:2 batch:7000 loss:2.3086 acc:0.1000(2799/28004)\n",
            "epoch:2 batch:7100 loss:2.3085 acc:0.1001(2844/28404)\n",
            "epoch:2 batch:7200 loss:2.3085 acc:0.1003(2890/28804)\n",
            "epoch:2 batch:7300 loss:2.3084 acc:0.1005(2934/29204)\n",
            "epoch:2 batch:7400 loss:2.3085 acc:0.1002(2965/29604)\n",
            "epoch:2 batch:7500 loss:2.3086 acc:0.1000(2999/30004)\n",
            "epoch:2 batch:7600 loss:2.3087 acc:0.1000(3039/30404)\n",
            "epoch:2 batch:7700 loss:2.3086 acc:0.1001(3084/30804)\n",
            "epoch:2 batch:7800 loss:2.3086 acc:0.1002(3127/31204)\n",
            "epoch:2 batch:7900 loss:2.3088 acc:0.1000(3160/31604)\n",
            "epoch:2 batch:8000 loss:2.3089 acc:0.1000(3201/32004)\n",
            "epoch:2 batch:8100 loss:2.3088 acc:0.1002(3246/32404)\n",
            "epoch:2 batch:8200 loss:2.3088 acc:0.1004(3293/32804)\n",
            "epoch:2 batch:8300 loss:2.3088 acc:0.1006(3340/33204)\n",
            "epoch:2 batch:8400 loss:2.3086 acc:0.1009(3391/33604)\n",
            "epoch:2 batch:8500 loss:2.3086 acc:0.1010(3434/34004)\n",
            "epoch:2 batch:8600 loss:2.3086 acc:0.1009(3470/34404)\n",
            "epoch:2 batch:8700 loss:2.3086 acc:0.1008(3508/34804)\n",
            "epoch:2 batch:8800 loss:2.3087 acc:0.1006(3543/35204)\n",
            "epoch:2 batch:8900 loss:2.3087 acc:0.1006(3580/35604)\n",
            "epoch:2 batch:9000 loss:2.3087 acc:0.1004(3615/36004)\n",
            "epoch:2 batch:9100 loss:2.3087 acc:0.1003(3652/36404)\n",
            "epoch:2 batch:9200 loss:2.3087 acc:0.1003(3691/36804)\n",
            "epoch:2 batch:9300 loss:2.3087 acc:0.1003(3732/37204)\n",
            "epoch:2 batch:9400 loss:2.3087 acc:0.1002(3768/37604)\n",
            "epoch:2 batch:9500 loss:2.3087 acc:0.1002(3808/38004)\n",
            "epoch:2 batch:9600 loss:2.3087 acc:0.1003(3851/38404)\n",
            "epoch:2 batch:9700 loss:2.3087 acc:0.1002(3887/38804)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-417da43e0cd4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d2c3a8234c40>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d2c3a8234c40>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTestSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/MachineLearningPractice/models/vgg.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}