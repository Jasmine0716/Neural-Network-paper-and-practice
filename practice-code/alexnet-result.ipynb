{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrnUbHsdJIo5",
        "outputId": "681de7e3-b919-4fd7-f73b-049ae955d794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQKCHOg6JQQ4",
        "outputId": "b3acfae4-37e9-4b40-99c4-ed440588c25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/MachineLearningPractice/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearningPractice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Z5pI9iPULv"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AGqGUWIHiqy",
        "outputId": "f4b327a5-784b-4257-b194-925bada55d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=57fabf22229d4b2134cabf509d99c9c6fbcd260e42b95ec2a5b4b27dc0037f84\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qurp-skGv6z",
        "outputId": "21de0449-8a15-4c36-8ff2-a80230b56998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "from models.alexnet import AlexNet\n",
        "from tools.DataLoader import LoadData"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/drive/My Drive/MachineLearningPractice/models/alexnet.ipynb\n",
            "importing Jupyter notebook from /content/drive/My Drive/MachineLearningPractice/tools/DataLoader.ipynb\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0HHnp-Pcos",
        "cellView": "both"
      },
      "source": [
        "class Solver(object):\n",
        "  def __init__(self,config):\n",
        "    self.ModelType = config.model\n",
        "    self.model = None\n",
        "    self.lr = config.lr\n",
        "    self.epochs = config.epoch\n",
        "    self.device = None\n",
        "    self.DataType = \"Cifar10\"\n",
        "    # self.train_batch_size = config.trainBatchSize\n",
        "    # self.test_batch_size = config.testBatchSize\n",
        "    self.criterion = None\n",
        "    self.optimizer = None\n",
        "    self.scheduler = None\n",
        "  \n",
        "  def DataLoading(self):\n",
        "    dataset = LoadData(self.DataType)\n",
        "    self.TrainSet = dataset['trainset']\n",
        "    self.TestSet = dataset['testset']\n",
        "    self.classes = dataset['classes']\n",
        "\n",
        "  def ModelLoading(self):\n",
        "    self.model = eval(self.ModelType)().to(self.device)\n",
        "    self.optimizer = optim.SGD(self.model.parameters(),lr=self.lr,momentum=0.9,weight_decay=0.0006)\n",
        "    # self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "    #scheduler\n",
        "    self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "    \n",
        "  def train(self,epoch):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_num = 0\n",
        "    \n",
        "    # kbar = pkbar.Kbar(target=len(self.TrainSet), epoch=epoch, num_epochs=self.epochs+1, width=10, always_stateful=False)\n",
        "    for batch_num, (data, target) in enumerate(self.TrainSet):\n",
        "      data, target = data.to(self.device), target.to(self.device)\n",
        "      self.optimizer.zero_grad()\n",
        "      output = self.model(data)\n",
        "      loss = self.criterion(output, target)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      prediction = torch.max(output, 1) #return a tuple\n",
        "      total_num += target.size(0)\n",
        "      # print(\"\\n\")\n",
        "      # print(total_correct)\n",
        "      # print(\"\\n\")\n",
        "      # print((prediction[1] == target).int())\n",
        "      total_correct += torch.sum((prediction[1] == target).int()).numpy()\n",
        "\n",
        "      # kbar.update(batch_num,values=[(\"Loss\",total_loss/(batch_num+1)),(\"Current Loss\",loss.item()),(\"Acc\",total_correct/total_num),(\"Correct\",total_correct),(\"Sum\",total_num)])\n",
        "      if (batch_num%100==0):\n",
        "          print(\"epoch:{:d} batch:{:d} loss:{:.4f} acc:{:.4f}({:d}/{:d})\".format(epoch,batch_num,total_loss/(batch_num+1),total_correct/total_num,total_correct,total_num))\n",
        "    return total_loss, total_correct/total_num\n",
        "\n",
        "  def test(self,epoch):\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    test_num = 0\n",
        "\n",
        "    # kbar = pkbar.Kbar(target=len(self.TestSet), epoch=epoch, num_epochs=self.epochs+1, width=10, always_stateful=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_num, (data, target) in enumerate(self.TestSet):\n",
        "        data, target = data.to(self.device), target.to(self.device)\n",
        "        output = self.model(data)\n",
        "        loss = self.criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        prediction = torch.max(output, 1) #return a tuple\n",
        "        test_num += target.size(0)\n",
        "        test_correct += torch.sum((prediction[1] == target).int()).numpy()\n",
        "\n",
        "        # kbar.update(batch_num,values=[(\"Loss\",test_loss/(batch_num+1)),(\"Acc\",test_correct/test_num),(\"Correct\",test_correct),(\"Sum\",test_num)])\n",
        "        if (batch_num%100==0):\n",
        "          print(\"epoch:{:d} batch:{:d} loss:{:.4f} acc:{:.4f}({:d}/{:d})\".format(epoch,batch_num,test_loss/(batch_num+1),test_correct/test_num,test_correct,test_num))\n",
        "    return test_loss, test_correct/test_num\n",
        "\n",
        "  def save(self):\n",
        "    model_path = \"model.pth\"\n",
        "    torch.save(self.model,model_path)\n",
        "    print(\"Saved in {}\".format(model_path))\n",
        "\n",
        "  # load model data??\n",
        "\n",
        "  def run(self):\n",
        "    self.DataLoading()\n",
        "    self.ModelLoading()\n",
        "    accuracy = 0\n",
        "    for epoch in range(1,self.epochs+1):\n",
        "      # scheduler\n",
        "      train_result = self.train(epoch)\n",
        "      print(train_result)\n",
        "      test_result = self.test(epoch)\n",
        "      best_accuracy = max(accuracy,test_result[1])\n",
        "      if epoch == self.epochs:\n",
        "        print(\"\\n==>Best Performance: %.3f%%\"%(accuracy*100))\n",
        "        self.save()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgjmC9DfARkO"
      },
      "source": [
        "def main():\n",
        "  parser = argparse.ArgumentParser(description=\"cifar-10 with PyTorch\")\n",
        "  parser.add_argument('--model',default='AlexNet',type=str,help='model type for training')\n",
        "  parser.add_argument('--lr',default=0.005,type=float,help='learning rate')\n",
        "  parser.add_argument('--epoch',default=3,type=int,help=\"epochs for training\")\n",
        "  parser.add_argument('--trainBatchSize',default=100,type=int,help=\"batch size for training\")\n",
        "  parser.add_argument('--testBatchSize',default=100,type=int,help=\"batche size for test\")\n",
        "  parser.add_argument('--cuda',default=torch.cuda.is_available(),type=bool,help='whether cuda is in use')\n",
        "  parser.add_argument('-f')\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  solver = Solver(args)\n",
        "  solver.run()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX7JRAgxQY7s",
        "outputId": "cc29ac26-dc83-4b8c-e9a5-88bd9816ec39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch:1 batch:0 loss:2.2980 acc:0.0000(0/4)\n",
            "epoch:1 batch:100 loss:2.3038 acc:0.0965(39/404)\n",
            "epoch:1 batch:200 loss:2.3037 acc:0.0908(73/804)\n",
            "epoch:1 batch:300 loss:2.3063 acc:0.0814(98/1204)\n",
            "epoch:1 batch:400 loss:2.3061 acc:0.0810(130/1604)\n",
            "epoch:1 batch:500 loss:2.3054 acc:0.0893(179/2004)\n",
            "epoch:1 batch:600 loss:2.3066 acc:0.0907(218/2404)\n",
            "epoch:1 batch:700 loss:2.3059 acc:0.0924(259/2804)\n",
            "epoch:1 batch:800 loss:2.3063 acc:0.0949(304/3204)\n",
            "epoch:1 batch:900 loss:2.3062 acc:0.0938(338/3604)\n",
            "epoch:1 batch:1000 loss:2.3056 acc:0.0949(380/4004)\n",
            "epoch:1 batch:1100 loss:2.3053 acc:0.0972(428/4404)\n",
            "epoch:1 batch:1200 loss:2.3054 acc:0.0985(473/4804)\n",
            "epoch:1 batch:1300 loss:2.3047 acc:0.0999(520/5204)\n",
            "epoch:1 batch:1400 loss:2.3023 acc:0.1030(577/5604)\n",
            "epoch:1 batch:1500 loss:2.3004 acc:0.1066(640/6004)\n",
            "epoch:1 batch:1600 loss:2.2986 acc:0.1067(683/6404)\n",
            "epoch:1 batch:1700 loss:2.2987 acc:0.1066(725/6804)\n",
            "epoch:1 batch:1800 loss:2.2993 acc:0.1044(752/7204)\n",
            "epoch:1 batch:1900 loss:2.2996 acc:0.1047(796/7604)\n",
            "epoch:1 batch:2000 loss:2.3001 acc:0.1039(832/8004)\n",
            "epoch:1 batch:2100 loss:2.3000 acc:0.1045(878/8404)\n",
            "epoch:1 batch:2200 loss:2.3003 acc:0.1050(924/8804)\n",
            "epoch:1 batch:2300 loss:2.3005 acc:0.1051(967/9204)\n",
            "epoch:1 batch:2400 loss:2.3008 acc:0.1038(997/9604)\n",
            "epoch:1 batch:2500 loss:2.3009 acc:0.1035(1035/10004)\n",
            "epoch:1 batch:2600 loss:2.3010 acc:0.1035(1077/10404)\n",
            "epoch:1 batch:2700 loss:2.3011 acc:0.1038(1121/10804)\n",
            "epoch:1 batch:2800 loss:2.3013 acc:0.1038(1163/11204)\n",
            "epoch:1 batch:2900 loss:2.3014 acc:0.1031(1196/11604)\n",
            "epoch:1 batch:3000 loss:2.3014 acc:0.1036(1244/12004)\n",
            "epoch:1 batch:3100 loss:2.3011 acc:0.1043(1294/12404)\n",
            "epoch:1 batch:3200 loss:2.3010 acc:0.1043(1335/12804)\n",
            "epoch:1 batch:3300 loss:2.3002 acc:0.1048(1384/13204)\n",
            "epoch:1 batch:3400 loss:2.2986 acc:0.1059(1440/13604)\n",
            "epoch:1 batch:3500 loss:2.2975 acc:0.1069(1497/14004)\n",
            "epoch:1 batch:3600 loss:2.2956 acc:0.1084(1562/14404)\n",
            "epoch:1 batch:3700 loss:2.2933 acc:0.1092(1616/14804)\n",
            "epoch:1 batch:3800 loss:2.2902 acc:0.1111(1689/15204)\n",
            "epoch:1 batch:3900 loss:2.2867 acc:0.1124(1754/15604)\n",
            "epoch:1 batch:4000 loss:2.2828 acc:0.1136(1818/16004)\n",
            "epoch:1 batch:4100 loss:2.2833 acc:0.1142(1874/16404)\n",
            "epoch:1 batch:4200 loss:2.2801 acc:0.1152(1935/16804)\n",
            "epoch:1 batch:4300 loss:2.2774 acc:0.1161(1998/17204)\n",
            "epoch:1 batch:4400 loss:2.2741 acc:0.1177(2072/17604)\n",
            "epoch:1 batch:4500 loss:2.2698 acc:0.1189(2140/18004)\n",
            "epoch:1 batch:4600 loss:2.2670 acc:0.1199(2206/18404)\n",
            "epoch:1 batch:4700 loss:2.2625 acc:0.1229(2311/18804)\n",
            "epoch:1 batch:4800 loss:2.2586 acc:0.1247(2395/19204)\n",
            "epoch:1 batch:4900 loss:2.2561 acc:0.1263(2476/19604)\n",
            "epoch:1 batch:5000 loss:2.2525 acc:0.1282(2564/20004)\n",
            "epoch:1 batch:5100 loss:2.2481 acc:0.1301(2655/20404)\n",
            "epoch:1 batch:5200 loss:2.2440 acc:0.1320(2747/20804)\n",
            "epoch:1 batch:5300 loss:2.2396 acc:0.1332(2824/21204)\n",
            "epoch:1 batch:5400 loss:2.2354 acc:0.1342(2899/21604)\n",
            "epoch:1 batch:5500 loss:2.2317 acc:0.1356(2984/22004)\n",
            "epoch:1 batch:5600 loss:2.2276 acc:0.1370(3069/22404)\n",
            "epoch:1 batch:5700 loss:2.2233 acc:0.1382(3151/22804)\n",
            "epoch:1 batch:5800 loss:2.2201 acc:0.1395(3238/23204)\n",
            "epoch:1 batch:5900 loss:2.2167 acc:0.1412(3334/23604)\n",
            "epoch:1 batch:6000 loss:2.2125 acc:0.1427(3426/24004)\n",
            "epoch:1 batch:6100 loss:2.2092 acc:0.1439(3512/24404)\n",
            "epoch:1 batch:6200 loss:2.2060 acc:0.1452(3602/24804)\n",
            "epoch:1 batch:6300 loss:2.2016 acc:0.1468(3700/25204)\n",
            "epoch:1 batch:6400 loss:2.1981 acc:0.1485(3801/25604)\n",
            "epoch:1 batch:6500 loss:2.1937 acc:0.1503(3909/26004)\n",
            "epoch:1 batch:6600 loss:2.1911 acc:0.1520(4014/26404)\n",
            "epoch:1 batch:6700 loss:2.1881 acc:0.1534(4113/26804)\n",
            "epoch:1 batch:6800 loss:2.1841 acc:0.1549(4215/27204)\n",
            "epoch:1 batch:6900 loss:2.1814 acc:0.1562(4311/27604)\n",
            "epoch:1 batch:7000 loss:2.1777 acc:0.1579(4421/28004)\n",
            "epoch:1 batch:7100 loss:2.1736 acc:0.1595(4531/28404)\n",
            "epoch:1 batch:7200 loss:2.1700 acc:0.1606(4626/28804)\n",
            "epoch:1 batch:7300 loss:2.1659 acc:0.1625(4746/29204)\n",
            "epoch:1 batch:7400 loss:2.1619 acc:0.1642(4862/29604)\n",
            "epoch:1 batch:7500 loss:2.1587 acc:0.1660(4982/30004)\n",
            "epoch:1 batch:7600 loss:2.1543 acc:0.1679(5105/30404)\n",
            "epoch:1 batch:7700 loss:2.1506 acc:0.1698(5229/30804)\n",
            "epoch:1 batch:7800 loss:2.1475 acc:0.1710(5337/31204)\n",
            "epoch:1 batch:7900 loss:2.1443 acc:0.1728(5460/31604)\n",
            "epoch:1 batch:8000 loss:2.1415 acc:0.1738(5563/32004)\n",
            "epoch:1 batch:8100 loss:2.1387 acc:0.1753(5680/32404)\n",
            "epoch:1 batch:8200 loss:2.1366 acc:0.1761(5778/32804)\n",
            "epoch:1 batch:8300 loss:2.1339 acc:0.1772(5885/33204)\n",
            "epoch:1 batch:8400 loss:2.1308 acc:0.1783(5992/33604)\n",
            "epoch:1 batch:8500 loss:2.1276 acc:0.1796(6106/34004)\n",
            "epoch:1 batch:8600 loss:2.1246 acc:0.1805(6211/34404)\n",
            "epoch:1 batch:8700 loss:2.1216 acc:0.1816(6322/34804)\n",
            "epoch:1 batch:8800 loss:2.1190 acc:0.1828(6437/35204)\n",
            "epoch:1 batch:8900 loss:2.1162 acc:0.1839(6546/35604)\n",
            "epoch:1 batch:9000 loss:2.1129 acc:0.1851(6663/36004)\n",
            "epoch:1 batch:9100 loss:2.1102 acc:0.1860(6770/36404)\n",
            "epoch:1 batch:9200 loss:2.1073 acc:0.1871(6885/36804)\n",
            "epoch:1 batch:9300 loss:2.1046 acc:0.1885(7012/37204)\n",
            "epoch:1 batch:9400 loss:2.1021 acc:0.1896(7130/37604)\n",
            "epoch:1 batch:9500 loss:2.0992 acc:0.1906(7243/38004)\n",
            "epoch:1 batch:9600 loss:2.0970 acc:0.1915(7356/38404)\n",
            "epoch:1 batch:9700 loss:2.0943 acc:0.1927(7478/38804)\n",
            "epoch:1 batch:9800 loss:2.0911 acc:0.1940(7606/39204)\n",
            "epoch:1 batch:9900 loss:2.0879 acc:0.1954(7738/39604)\n",
            "epoch:1 batch:10000 loss:2.0852 acc:0.1968(7872/40004)\n",
            "epoch:1 batch:10100 loss:2.0817 acc:0.1981(8003/40404)\n",
            "epoch:1 batch:10200 loss:2.0788 acc:0.1992(8130/40804)\n",
            "epoch:1 batch:10300 loss:2.0762 acc:0.2002(8250/41204)\n",
            "epoch:1 batch:10400 loss:2.0738 acc:0.2010(8361/41604)\n",
            "epoch:1 batch:10500 loss:2.0708 acc:0.2024(8501/42004)\n",
            "epoch:1 batch:10600 loss:2.0680 acc:0.2031(8613/42404)\n",
            "epoch:1 batch:10700 loss:2.0651 acc:0.2040(8734/42804)\n",
            "epoch:1 batch:10800 loss:2.0626 acc:0.2050(8856/43204)\n",
            "epoch:1 batch:10900 loss:2.0604 acc:0.2064(9000/43604)\n",
            "epoch:1 batch:11000 loss:2.0585 acc:0.2071(9113/44004)\n",
            "epoch:1 batch:11100 loss:2.0559 acc:0.2079(9232/44404)\n",
            "epoch:1 batch:11200 loss:2.0526 acc:0.2096(9390/44804)\n",
            "epoch:1 batch:11300 loss:2.0509 acc:0.2104(9510/45204)\n",
            "epoch:1 batch:11400 loss:2.0485 acc:0.2114(9641/45604)\n",
            "epoch:1 batch:11500 loss:2.0456 acc:0.2129(9792/46004)\n",
            "epoch:1 batch:11600 loss:2.0430 acc:0.2138(9921/46404)\n",
            "epoch:1 batch:11700 loss:2.0405 acc:0.2150(10061/46804)\n",
            "epoch:1 batch:11800 loss:2.0385 acc:0.2160(10196/47204)\n",
            "epoch:1 batch:11900 loss:2.0365 acc:0.2165(10307/47604)\n",
            "epoch:1 batch:12000 loss:2.0337 acc:0.2177(10449/48004)\n",
            "epoch:1 batch:12100 loss:2.0320 acc:0.2188(10592/48404)\n",
            "epoch:1 batch:12200 loss:2.0300 acc:0.2196(10717/48804)\n",
            "epoch:1 batch:12300 loss:2.0278 acc:0.2206(10855/49204)\n",
            "epoch:1 batch:12400 loss:2.0255 acc:0.2217(10997/49604)\n",
            "(25286.661422252655, 0.22262)\n",
            "epoch:1 batch:0 loss:1.4605 acc:0.7500(3/4)\n",
            "epoch:1 batch:100 loss:1.8236 acc:0.3416(138/404)\n",
            "epoch:1 batch:200 loss:1.8404 acc:0.3197(257/804)\n",
            "epoch:1 batch:300 loss:1.8291 acc:0.3214(387/1204)\n",
            "epoch:1 batch:400 loss:1.8177 acc:0.3323(533/1604)\n",
            "epoch:1 batch:500 loss:1.8376 acc:0.3278(657/2004)\n",
            "epoch:1 batch:600 loss:1.8342 acc:0.3295(792/2404)\n",
            "epoch:1 batch:700 loss:1.8316 acc:0.3306(927/2804)\n",
            "epoch:1 batch:800 loss:1.8307 acc:0.3327(1066/3204)\n",
            "epoch:1 batch:900 loss:1.8423 acc:0.3305(1191/3604)\n",
            "epoch:1 batch:1000 loss:1.8364 acc:0.3354(1343/4004)\n",
            "epoch:1 batch:1100 loss:1.8384 acc:0.3331(1467/4404)\n",
            "epoch:1 batch:1200 loss:1.8302 acc:0.3333(1601/4804)\n",
            "epoch:1 batch:1300 loss:1.8413 acc:0.3328(1732/5204)\n",
            "epoch:1 batch:1400 loss:1.8370 acc:0.3344(1874/5604)\n",
            "epoch:1 batch:1500 loss:1.8393 acc:0.3349(2011/6004)\n",
            "epoch:1 batch:1600 loss:1.8403 acc:0.3335(2136/6404)\n",
            "epoch:1 batch:1700 loss:1.8390 acc:0.3339(2272/6804)\n",
            "epoch:1 batch:1800 loss:1.8401 acc:0.3326(2396/7204)\n",
            "epoch:1 batch:1900 loss:1.8375 acc:0.3338(2538/7604)\n",
            "epoch:1 batch:2000 loss:1.8416 acc:0.3323(2660/8004)\n",
            "epoch:1 batch:2100 loss:1.8449 acc:0.3302(2775/8404)\n",
            "epoch:1 batch:2200 loss:1.8411 acc:0.3318(2921/8804)\n",
            "epoch:1 batch:2300 loss:1.8410 acc:0.3326(3061/9204)\n",
            "epoch:1 batch:2400 loss:1.8427 acc:0.3311(3180/9604)\n",
            "epoch:1 batch:2500 loss:1.8431 acc:0.3307(3308/10004)\n",
            "epoch:1 batch:2600 loss:1.8432 acc:0.3312(3446/10404)\n",
            "epoch:1 batch:2700 loss:1.8433 acc:0.3312(3578/10804)\n",
            "epoch:1 batch:2800 loss:1.8449 acc:0.3308(3706/11204)\n",
            "epoch:1 batch:2900 loss:1.8450 acc:0.3314(3845/11604)\n",
            "epoch:1 batch:3000 loss:1.8443 acc:0.3311(3974/12004)\n",
            "epoch:1 batch:3100 loss:1.8440 acc:0.3309(4104/12404)\n",
            "epoch:1 batch:3200 loss:1.8437 acc:0.3312(4241/12804)\n",
            "epoch:1 batch:3300 loss:1.8418 acc:0.3313(4374/13204)\n",
            "epoch:1 batch:3400 loss:1.8427 acc:0.3303(4493/13604)\n",
            "epoch:1 batch:3500 loss:1.8428 acc:0.3310(4635/14004)\n",
            "epoch:1 batch:3600 loss:1.8448 acc:0.3309(4766/14404)\n",
            "epoch:1 batch:3700 loss:1.8451 acc:0.3304(4891/14804)\n",
            "epoch:1 batch:3800 loss:1.8471 acc:0.3297(5013/15204)\n",
            "epoch:1 batch:3900 loss:1.8477 acc:0.3285(5126/15604)\n",
            "epoch:1 batch:4000 loss:1.8488 acc:0.3274(5239/16004)\n",
            "epoch:1 batch:4100 loss:1.8493 acc:0.3269(5362/16404)\n",
            "epoch:1 batch:4200 loss:1.8497 acc:0.3266(5488/16804)\n",
            "epoch:1 batch:4300 loss:1.8500 acc:0.3261(5610/17204)\n",
            "epoch:1 batch:4400 loss:1.8491 acc:0.3260(5739/17604)\n",
            "epoch:1 batch:4500 loss:1.8500 acc:0.3264(5877/18004)\n",
            "epoch:1 batch:4600 loss:1.8507 acc:0.3259(5998/18404)\n",
            "epoch:1 batch:4700 loss:1.8518 acc:0.3251(6114/18804)\n",
            "epoch:1 batch:4800 loss:1.8516 acc:0.3259(6258/19204)\n",
            "epoch:1 batch:4900 loss:1.8501 acc:0.3261(6392/19604)\n",
            "epoch:1 batch:5000 loss:1.8514 acc:0.3257(6516/20004)\n",
            "epoch:1 batch:5100 loss:1.8520 acc:0.3252(6635/20404)\n",
            "epoch:1 batch:5200 loss:1.8511 acc:0.3255(6771/20804)\n",
            "epoch:1 batch:5300 loss:1.8498 acc:0.3256(6904/21204)\n",
            "epoch:1 batch:5400 loss:1.8476 acc:0.3264(7052/21604)\n",
            "epoch:1 batch:5500 loss:1.8479 acc:0.3262(7177/22004)\n",
            "epoch:1 batch:5600 loss:1.8477 acc:0.3262(7308/22404)\n",
            "epoch:1 batch:5700 loss:1.8467 acc:0.3260(7434/22804)\n",
            "epoch:1 batch:5800 loss:1.8464 acc:0.3256(7556/23204)\n",
            "epoch:1 batch:5900 loss:1.8450 acc:0.3260(7696/23604)\n",
            "epoch:1 batch:6000 loss:1.8439 acc:0.3266(7839/24004)\n",
            "epoch:1 batch:6100 loss:1.8426 acc:0.3266(7970/24404)\n",
            "epoch:1 batch:6200 loss:1.8413 acc:0.3262(8090/24804)\n",
            "epoch:1 batch:6300 loss:1.8413 acc:0.3265(8230/25204)\n",
            "epoch:1 batch:6400 loss:1.8416 acc:0.3258(8341/25604)\n",
            "epoch:1 batch:6500 loss:1.8420 acc:0.3259(8475/26004)\n",
            "epoch:1 batch:6600 loss:1.8404 acc:0.3265(8622/26404)\n",
            "epoch:1 batch:6700 loss:1.8387 acc:0.3272(8770/26804)\n",
            "epoch:1 batch:6800 loss:1.8390 acc:0.3272(8902/27204)\n",
            "epoch:1 batch:6900 loss:1.8403 acc:0.3270(9026/27604)\n",
            "epoch:1 batch:7000 loss:1.8396 acc:0.3272(9163/28004)\n",
            "epoch:1 batch:7100 loss:1.8401 acc:0.3272(9295/28404)\n",
            "epoch:1 batch:7200 loss:1.8402 acc:0.3273(9428/28804)\n",
            "epoch:1 batch:7300 loss:1.8387 acc:0.3277(9570/29204)\n",
            "epoch:1 batch:7400 loss:1.8375 acc:0.3281(9712/29604)\n",
            "epoch:1 batch:7500 loss:1.8367 acc:0.3279(9838/30004)\n",
            "epoch:1 batch:7600 loss:1.8365 acc:0.3279(9970/30404)\n",
            "epoch:1 batch:7700 loss:1.8378 acc:0.3275(10088/30804)\n",
            "epoch:1 batch:7800 loss:1.8371 acc:0.3280(10236/31204)\n",
            "epoch:1 batch:7900 loss:1.8374 acc:0.3286(10384/31604)\n",
            "epoch:1 batch:8000 loss:1.8373 acc:0.3287(10520/32004)\n",
            "epoch:1 batch:8100 loss:1.8374 acc:0.3287(10650/32404)\n",
            "epoch:1 batch:8200 loss:1.8383 acc:0.3284(10772/32804)\n",
            "epoch:1 batch:8300 loss:1.8387 acc:0.3285(10906/33204)\n",
            "epoch:1 batch:8400 loss:1.8390 acc:0.3282(11029/33604)\n",
            "epoch:1 batch:8500 loss:1.8400 acc:0.3277(11144/34004)\n",
            "epoch:1 batch:8600 loss:1.8401 acc:0.3278(11277/34404)\n",
            "epoch:1 batch:8700 loss:1.8396 acc:0.3279(11413/34804)\n",
            "epoch:1 batch:8800 loss:1.8396 acc:0.3276(11533/35204)\n",
            "epoch:1 batch:8900 loss:1.8399 acc:0.3276(11663/35604)\n",
            "epoch:1 batch:9000 loss:1.8400 acc:0.3275(11790/36004)\n",
            "epoch:1 batch:9100 loss:1.8405 acc:0.3274(11920/36404)\n",
            "epoch:1 batch:9200 loss:1.8402 acc:0.3274(12049/36804)\n",
            "epoch:1 batch:9300 loss:1.8401 acc:0.3274(12179/37204)\n",
            "epoch:1 batch:9400 loss:1.8401 acc:0.3274(12311/37604)\n",
            "epoch:1 batch:9500 loss:1.8406 acc:0.3273(12440/38004)\n",
            "epoch:1 batch:9600 loss:1.8410 acc:0.3275(12577/38404)\n",
            "epoch:1 batch:9700 loss:1.8407 acc:0.3275(12707/38804)\n",
            "epoch:1 batch:9800 loss:1.8404 acc:0.3274(12837/39204)\n",
            "epoch:1 batch:9900 loss:1.8410 acc:0.3274(12965/39604)\n",
            "epoch:1 batch:10000 loss:1.8409 acc:0.3273(13094/40004)\n",
            "epoch:1 batch:10100 loss:1.8407 acc:0.3274(13228/40404)\n",
            "epoch:1 batch:10200 loss:1.8405 acc:0.3276(13368/40804)\n",
            "epoch:1 batch:10300 loss:1.8411 acc:0.3273(13487/41204)\n",
            "epoch:1 batch:10400 loss:1.8409 acc:0.3274(13623/41604)\n",
            "epoch:1 batch:10500 loss:1.8407 acc:0.3275(13758/42004)\n",
            "epoch:1 batch:10600 loss:1.8406 acc:0.3274(13882/42404)\n",
            "epoch:1 batch:10700 loss:1.8404 acc:0.3274(14013/42804)\n",
            "epoch:1 batch:10800 loss:1.8405 acc:0.3276(14152/43204)\n",
            "epoch:1 batch:10900 loss:1.8408 acc:0.3276(14283/43604)\n",
            "epoch:1 batch:11000 loss:1.8409 acc:0.3274(14406/44004)\n",
            "epoch:1 batch:11100 loss:1.8417 acc:0.3271(14524/44404)\n",
            "epoch:1 batch:11200 loss:1.8421 acc:0.3270(14652/44804)\n",
            "epoch:1 batch:11300 loss:1.8413 acc:0.3271(14784/45204)\n",
            "epoch:1 batch:11400 loss:1.8410 acc:0.3272(14921/45604)\n",
            "epoch:1 batch:11500 loss:1.8406 acc:0.3271(15048/46004)\n",
            "epoch:1 batch:11600 loss:1.8403 acc:0.3272(15183/46404)\n",
            "epoch:1 batch:11700 loss:1.8409 acc:0.3268(15297/46804)\n",
            "epoch:1 batch:11800 loss:1.8410 acc:0.3271(15439/47204)\n",
            "epoch:1 batch:11900 loss:1.8408 acc:0.3271(15570/47604)\n",
            "epoch:1 batch:12000 loss:1.8406 acc:0.3270(15699/48004)\n",
            "epoch:1 batch:12100 loss:1.8404 acc:0.3270(15830/48404)\n",
            "epoch:1 batch:12200 loss:1.8410 acc:0.3271(15963/48804)\n",
            "epoch:1 batch:12300 loss:1.8406 acc:0.3273(16106/49204)\n",
            "epoch:1 batch:12400 loss:1.8400 acc:0.3273(16234/49604)\n",
            "epoch:2 batch:0 loss:2.9206 acc:0.2500(1/4)\n",
            "epoch:2 batch:100 loss:1.7125 acc:0.3564(144/404)\n",
            "epoch:2 batch:200 loss:1.7767 acc:0.3333(268/804)\n",
            "epoch:2 batch:300 loss:1.7141 acc:0.3630(437/1204)\n",
            "epoch:2 batch:400 loss:1.7270 acc:0.3597(577/1604)\n",
            "epoch:2 batch:500 loss:1.7320 acc:0.3543(710/2004)\n",
            "epoch:2 batch:600 loss:1.7185 acc:0.3598(865/2404)\n",
            "epoch:2 batch:700 loss:1.7203 acc:0.3588(1006/2804)\n",
            "epoch:2 batch:800 loss:1.7127 acc:0.3596(1152/3204)\n",
            "epoch:2 batch:900 loss:1.7045 acc:0.3618(1304/3604)\n",
            "epoch:2 batch:1000 loss:1.6951 acc:0.3606(1444/4004)\n",
            "epoch:2 batch:1100 loss:1.6923 acc:0.3644(1605/4404)\n",
            "epoch:2 batch:1200 loss:1.6962 acc:0.3641(1749/4804)\n",
            "epoch:2 batch:1300 loss:1.6986 acc:0.3609(1878/5204)\n",
            "epoch:2 batch:1400 loss:1.6963 acc:0.3619(2028/5604)\n",
            "epoch:2 batch:1500 loss:1.6959 acc:0.3623(2175/6004)\n",
            "epoch:2 batch:1600 loss:1.6953 acc:0.3621(2319/6404)\n",
            "epoch:2 batch:1700 loss:1.6979 acc:0.3611(2457/6804)\n",
            "epoch:2 batch:1800 loss:1.6980 acc:0.3606(2598/7204)\n",
            "epoch:2 batch:1900 loss:1.7010 acc:0.3606(2742/7604)\n",
            "epoch:2 batch:2000 loss:1.6958 acc:0.3618(2896/8004)\n",
            "epoch:2 batch:2100 loss:1.6959 acc:0.3623(3045/8404)\n",
            "epoch:2 batch:2200 loss:1.6939 acc:0.3623(3190/8804)\n",
            "epoch:2 batch:2300 loss:1.6963 acc:0.3630(3341/9204)\n",
            "epoch:2 batch:2400 loss:1.6972 acc:0.3626(3482/9604)\n",
            "epoch:2 batch:2500 loss:1.6945 acc:0.3649(3650/10004)\n",
            "epoch:2 batch:2600 loss:1.6925 acc:0.3649(3796/10404)\n",
            "epoch:2 batch:2700 loss:1.6915 acc:0.3645(3938/10804)\n",
            "epoch:2 batch:2800 loss:1.6927 acc:0.3634(4072/11204)\n",
            "epoch:2 batch:2900 loss:1.6873 acc:0.3661(4248/11604)\n",
            "epoch:2 batch:3000 loss:1.6856 acc:0.3660(4394/12004)\n",
            "epoch:2 batch:3100 loss:1.6830 acc:0.3658(4538/12404)\n",
            "epoch:2 batch:3200 loss:1.6827 acc:0.3658(4684/12804)\n",
            "epoch:2 batch:3300 loss:1.6803 acc:0.3661(4834/13204)\n",
            "epoch:2 batch:3400 loss:1.6786 acc:0.3671(4994/13604)\n",
            "epoch:2 batch:3500 loss:1.6799 acc:0.3673(5144/14004)\n",
            "epoch:2 batch:3600 loss:1.6770 acc:0.3685(5308/14404)\n",
            "epoch:2 batch:3700 loss:1.6770 acc:0.3688(5460/14804)\n",
            "epoch:2 batch:3800 loss:1.6749 acc:0.3692(5614/15204)\n",
            "epoch:2 batch:3900 loss:1.6740 acc:0.3698(5770/15604)\n",
            "epoch:2 batch:4000 loss:1.6719 acc:0.3703(5927/16004)\n",
            "epoch:2 batch:4100 loss:1.6713 acc:0.3708(6082/16404)\n",
            "epoch:2 batch:4200 loss:1.6707 acc:0.3712(6238/16804)\n",
            "epoch:2 batch:4300 loss:1.6680 acc:0.3725(6409/17204)\n",
            "epoch:2 batch:4400 loss:1.6678 acc:0.3729(6564/17604)\n",
            "epoch:2 batch:4500 loss:1.6653 acc:0.3741(6735/18004)\n",
            "epoch:2 batch:4600 loss:1.6639 acc:0.3748(6898/18404)\n",
            "epoch:2 batch:4700 loss:1.6638 acc:0.3748(7048/18804)\n",
            "epoch:2 batch:4800 loss:1.6627 acc:0.3750(7201/19204)\n",
            "epoch:2 batch:4900 loss:1.6624 acc:0.3750(7352/19604)\n",
            "epoch:2 batch:5000 loss:1.6611 acc:0.3759(7519/20004)\n",
            "epoch:2 batch:5100 loss:1.6623 acc:0.3752(7656/20404)\n",
            "epoch:2 batch:5200 loss:1.6622 acc:0.3752(7806/20804)\n",
            "epoch:2 batch:5300 loss:1.6615 acc:0.3755(7963/21204)\n",
            "epoch:2 batch:5400 loss:1.6617 acc:0.3754(8111/21604)\n",
            "epoch:2 batch:5500 loss:1.6614 acc:0.3759(8272/22004)\n",
            "epoch:2 batch:5600 loss:1.6599 acc:0.3768(8442/22404)\n",
            "epoch:2 batch:5700 loss:1.6589 acc:0.3771(8600/22804)\n",
            "epoch:2 batch:5800 loss:1.6579 acc:0.3777(8764/23204)\n",
            "epoch:2 batch:5900 loss:1.6587 acc:0.3782(8928/23604)\n",
            "epoch:2 batch:6000 loss:1.6568 acc:0.3794(9107/24004)\n",
            "epoch:2 batch:6100 loss:1.6555 acc:0.3803(9280/24404)\n",
            "epoch:2 batch:6200 loss:1.6547 acc:0.3805(9438/24804)\n",
            "epoch:2 batch:6300 loss:1.6528 acc:0.3808(9597/25204)\n",
            "epoch:2 batch:6400 loss:1.6527 acc:0.3806(9746/25604)\n",
            "epoch:2 batch:6500 loss:1.6503 acc:0.3821(9937/26004)\n",
            "epoch:2 batch:6600 loss:1.6494 acc:0.3821(10088/26404)\n",
            "epoch:2 batch:6700 loss:1.6485 acc:0.3826(10255/26804)\n",
            "epoch:2 batch:6800 loss:1.6489 acc:0.3823(10401/27204)\n",
            "epoch:2 batch:6900 loss:1.6480 acc:0.3823(10554/27604)\n",
            "epoch:2 batch:7000 loss:1.6462 acc:0.3829(10722/28004)\n",
            "epoch:2 batch:7100 loss:1.6459 acc:0.3833(10886/28404)\n",
            "epoch:2 batch:7200 loss:1.6453 acc:0.3838(11054/28804)\n",
            "epoch:2 batch:7300 loss:1.6442 acc:0.3843(11223/29204)\n",
            "epoch:2 batch:7400 loss:1.6432 acc:0.3846(11386/29604)\n",
            "epoch:2 batch:7500 loss:1.6424 acc:0.3854(11563/30004)\n",
            "epoch:2 batch:7600 loss:1.6423 acc:0.3856(11724/30404)\n",
            "epoch:2 batch:7700 loss:1.6424 acc:0.3858(11884/30804)\n",
            "epoch:2 batch:7800 loss:1.6416 acc:0.3861(12049/31204)\n",
            "epoch:2 batch:7900 loss:1.6415 acc:0.3862(12206/31604)\n",
            "epoch:2 batch:8000 loss:1.6399 acc:0.3864(12367/32004)\n",
            "epoch:2 batch:8100 loss:1.6392 acc:0.3871(12544/32404)\n",
            "epoch:2 batch:8200 loss:1.6375 acc:0.3875(12713/32804)\n",
            "epoch:2 batch:8300 loss:1.6367 acc:0.3877(12872/33204)\n",
            "epoch:2 batch:8400 loss:1.6369 acc:0.3876(13025/33604)\n",
            "epoch:2 batch:8500 loss:1.6348 acc:0.3882(13202/34004)\n",
            "epoch:2 batch:8600 loss:1.6345 acc:0.3881(13352/34404)\n",
            "epoch:2 batch:8700 loss:1.6335 acc:0.3887(13529/34804)\n",
            "epoch:2 batch:8800 loss:1.6316 acc:0.3897(13718/35204)\n",
            "epoch:2 batch:8900 loss:1.6303 acc:0.3899(13883/35604)\n",
            "epoch:2 batch:9000 loss:1.6294 acc:0.3904(14056/36004)\n",
            "epoch:2 batch:9100 loss:1.6278 acc:0.3912(14242/36404)\n",
            "epoch:2 batch:9200 loss:1.6270 acc:0.3916(14412/36804)\n",
            "epoch:2 batch:9300 loss:1.6268 acc:0.3915(14564/37204)\n",
            "epoch:2 batch:9400 loss:1.6262 acc:0.3921(14744/37604)\n",
            "epoch:2 batch:9500 loss:1.6247 acc:0.3930(14936/38004)\n",
            "epoch:2 batch:9600 loss:1.6245 acc:0.3935(15111/38404)\n",
            "epoch:2 batch:9700 loss:1.6230 acc:0.3940(15289/38804)\n",
            "epoch:2 batch:9800 loss:1.6226 acc:0.3943(15458/39204)\n",
            "epoch:2 batch:9900 loss:1.6225 acc:0.3946(15627/39604)\n",
            "epoch:2 batch:10000 loss:1.6213 acc:0.3954(15816/40004)\n",
            "epoch:2 batch:10100 loss:1.6213 acc:0.3953(15973/40404)\n",
            "epoch:2 batch:10200 loss:1.6193 acc:0.3963(16170/40804)\n",
            "epoch:2 batch:10300 loss:1.6166 acc:0.3974(16376/41204)\n",
            "epoch:2 batch:10400 loss:1.6166 acc:0.3975(16538/41604)\n",
            "epoch:2 batch:10500 loss:1.6149 acc:0.3982(16724/42004)\n",
            "epoch:2 batch:10600 loss:1.6141 acc:0.3986(16903/42404)\n",
            "epoch:2 batch:10700 loss:1.6127 acc:0.3993(17092/42804)\n",
            "epoch:2 batch:10800 loss:1.6115 acc:0.4001(17287/43204)\n",
            "epoch:2 batch:10900 loss:1.6102 acc:0.4007(17472/43604)\n",
            "epoch:2 batch:11000 loss:1.6088 acc:0.4013(17658/44004)\n",
            "epoch:2 batch:11100 loss:1.6075 acc:0.4019(17847/44404)\n",
            "epoch:2 batch:11200 loss:1.6056 acc:0.4027(18043/44804)\n",
            "epoch:2 batch:11300 loss:1.6052 acc:0.4029(18213/45204)\n",
            "epoch:2 batch:11400 loss:1.6054 acc:0.4031(18384/45604)\n",
            "epoch:2 batch:11500 loss:1.6044 acc:0.4037(18574/46004)\n",
            "epoch:2 batch:11600 loss:1.6036 acc:0.4044(18764/46404)\n",
            "epoch:2 batch:11700 loss:1.6035 acc:0.4045(18933/46804)\n",
            "epoch:2 batch:11800 loss:1.6023 acc:0.4052(19125/47204)\n",
            "epoch:2 batch:11900 loss:1.6023 acc:0.4050(19282/47604)\n",
            "epoch:2 batch:12000 loss:1.6015 acc:0.4057(19476/48004)\n",
            "epoch:2 batch:12100 loss:1.6001 acc:0.4064(19671/48404)\n",
            "epoch:2 batch:12200 loss:1.5985 acc:0.4071(19866/48804)\n",
            "epoch:2 batch:12300 loss:1.5980 acc:0.4071(20033/49204)\n",
            "epoch:2 batch:12400 loss:1.5969 acc:0.4077(20223/49604)\n",
            "(19949.48828601837, 0.40818)\n",
            "epoch:2 batch:0 loss:1.0974 acc:0.5000(2/4)\n",
            "epoch:2 batch:100 loss:1.5253 acc:0.4381(177/404)\n",
            "epoch:2 batch:200 loss:1.5579 acc:0.4279(344/804)\n",
            "epoch:2 batch:300 loss:1.5165 acc:0.4452(536/1204)\n",
            "epoch:2 batch:400 loss:1.5108 acc:0.4589(736/1604)\n",
            "epoch:2 batch:500 loss:1.5292 acc:0.4526(907/2004)\n",
            "epoch:2 batch:600 loss:1.5307 acc:0.4538(1091/2404)\n",
            "epoch:2 batch:700 loss:1.5341 acc:0.4554(1277/2804)\n",
            "epoch:2 batch:800 loss:1.5348 acc:0.4554(1459/3204)\n",
            "epoch:2 batch:900 loss:1.5434 acc:0.4517(1628/3604)\n",
            "epoch:2 batch:1000 loss:1.5395 acc:0.4553(1823/4004)\n",
            "epoch:2 batch:1100 loss:1.5416 acc:0.4539(1999/4404)\n",
            "epoch:2 batch:1200 loss:1.5427 acc:0.4540(2181/4804)\n",
            "epoch:2 batch:1300 loss:1.5545 acc:0.4500(2342/5204)\n",
            "epoch:2 batch:1400 loss:1.5549 acc:0.4495(2519/5604)\n",
            "epoch:2 batch:1500 loss:1.5505 acc:0.4485(2693/6004)\n",
            "epoch:2 batch:1600 loss:1.5496 acc:0.4491(2876/6404)\n",
            "epoch:2 batch:1700 loss:1.5459 acc:0.4490(3055/6804)\n",
            "epoch:2 batch:1800 loss:1.5480 acc:0.4474(3223/7204)\n",
            "epoch:2 batch:1900 loss:1.5462 acc:0.4487(3412/7604)\n",
            "epoch:2 batch:2000 loss:1.5494 acc:0.4477(3583/8004)\n",
            "epoch:2 batch:2100 loss:1.5475 acc:0.4498(3780/8404)\n",
            "epoch:2 batch:2200 loss:1.5485 acc:0.4482(3946/8804)\n",
            "epoch:2 batch:2300 loss:1.5519 acc:0.4472(4116/9204)\n",
            "epoch:2 batch:2400 loss:1.5539 acc:0.4466(4289/9604)\n",
            "epoch:2 batch:2500 loss:1.5512 acc:0.4477(4479/10004)\n",
            "epoch:2 batch:2600 loss:1.5547 acc:0.4473(4654/10404)\n",
            "epoch:2 batch:2700 loss:1.5551 acc:0.4473(4833/10804)\n",
            "epoch:2 batch:2800 loss:1.5545 acc:0.4474(5013/11204)\n",
            "epoch:2 batch:2900 loss:1.5522 acc:0.4482(5201/11604)\n",
            "epoch:2 batch:3000 loss:1.5537 acc:0.4469(5365/12004)\n",
            "epoch:2 batch:3100 loss:1.5556 acc:0.4459(5531/12404)\n",
            "epoch:2 batch:3200 loss:1.5569 acc:0.4463(5714/12804)\n",
            "epoch:2 batch:3300 loss:1.5552 acc:0.4468(5900/13204)\n",
            "epoch:2 batch:3400 loss:1.5563 acc:0.4461(6069/13604)\n",
            "epoch:2 batch:3500 loss:1.5568 acc:0.4462(6249/14004)\n",
            "epoch:2 batch:3600 loss:1.5581 acc:0.4460(6424/14404)\n",
            "epoch:2 batch:3700 loss:1.5589 acc:0.4447(6583/14804)\n",
            "epoch:2 batch:3800 loss:1.5600 acc:0.4445(6758/15204)\n",
            "epoch:2 batch:3900 loss:1.5595 acc:0.4448(6940/15604)\n",
            "epoch:2 batch:4000 loss:1.5613 acc:0.4440(7106/16004)\n",
            "epoch:2 batch:4100 loss:1.5620 acc:0.4439(7282/16404)\n",
            "epoch:2 batch:4200 loss:1.5618 acc:0.4440(7461/16804)\n",
            "epoch:2 batch:4300 loss:1.5599 acc:0.4450(7656/17204)\n",
            "epoch:2 batch:4400 loss:1.5586 acc:0.4455(7843/17604)\n",
            "epoch:2 batch:4500 loss:1.5602 acc:0.4445(8002/18004)\n",
            "epoch:2 batch:4600 loss:1.5599 acc:0.4440(8172/18404)\n",
            "epoch:2 batch:4700 loss:1.5589 acc:0.4445(8358/18804)\n",
            "epoch:2 batch:4800 loss:1.5578 acc:0.4448(8541/19204)\n",
            "epoch:2 batch:4900 loss:1.5555 acc:0.4451(8726/19604)\n",
            "epoch:2 batch:5000 loss:1.5545 acc:0.4452(8906/20004)\n",
            "epoch:2 batch:5100 loss:1.5556 acc:0.4453(9086/20404)\n",
            "epoch:2 batch:5200 loss:1.5562 acc:0.4451(9260/20804)\n",
            "epoch:2 batch:5300 loss:1.5557 acc:0.4455(9446/21204)\n",
            "epoch:2 batch:5400 loss:1.5560 acc:0.4450(9614/21604)\n",
            "epoch:2 batch:5500 loss:1.5554 acc:0.4453(9799/22004)\n",
            "epoch:2 batch:5600 loss:1.5544 acc:0.4452(9974/22404)\n",
            "epoch:2 batch:5700 loss:1.5552 acc:0.4448(10144/22804)\n",
            "epoch:2 batch:5800 loss:1.5547 acc:0.4448(10320/23204)\n",
            "epoch:2 batch:5900 loss:1.5544 acc:0.4448(10500/23604)\n",
            "epoch:2 batch:6000 loss:1.5522 acc:0.4456(10696/24004)\n",
            "epoch:2 batch:6100 loss:1.5513 acc:0.4463(10891/24404)\n",
            "epoch:2 batch:6200 loss:1.5502 acc:0.4468(11083/24804)\n",
            "epoch:2 batch:6300 loss:1.5492 acc:0.4475(11279/25204)\n",
            "epoch:2 batch:6400 loss:1.5498 acc:0.4474(11455/25604)\n",
            "epoch:2 batch:6500 loss:1.5484 acc:0.4477(11642/26004)\n",
            "epoch:2 batch:6600 loss:1.5471 acc:0.4478(11825/26404)\n",
            "epoch:2 batch:6700 loss:1.5473 acc:0.4477(12001/26804)\n",
            "epoch:2 batch:6800 loss:1.5481 acc:0.4474(12171/27204)\n",
            "epoch:2 batch:6900 loss:1.5480 acc:0.4474(12351/27604)\n",
            "epoch:2 batch:7000 loss:1.5471 acc:0.4480(12546/28004)\n",
            "epoch:2 batch:7100 loss:1.5468 acc:0.4483(12733/28404)\n",
            "epoch:2 batch:7200 loss:1.5466 acc:0.4482(12910/28804)\n",
            "epoch:2 batch:7300 loss:1.5454 acc:0.4488(13107/29204)\n",
            "epoch:2 batch:7400 loss:1.5442 acc:0.4490(13292/29604)\n",
            "epoch:2 batch:7500 loss:1.5439 acc:0.4493(13481/30004)\n",
            "epoch:2 batch:7600 loss:1.5443 acc:0.4491(13654/30404)\n",
            "epoch:2 batch:7700 loss:1.5448 acc:0.4491(13834/30804)\n",
            "epoch:2 batch:7800 loss:1.5449 acc:0.4492(14016/31204)\n",
            "epoch:2 batch:7900 loss:1.5454 acc:0.4488(14185/31604)\n",
            "epoch:2 batch:8000 loss:1.5451 acc:0.4492(14375/32004)\n",
            "epoch:2 batch:8100 loss:1.5469 acc:0.4487(14539/32404)\n",
            "epoch:2 batch:8200 loss:1.5472 acc:0.4490(14729/32804)\n",
            "epoch:2 batch:8300 loss:1.5478 acc:0.4488(14902/33204)\n",
            "epoch:2 batch:8400 loss:1.5490 acc:0.4486(15075/33604)\n",
            "epoch:2 batch:8500 loss:1.5500 acc:0.4484(15246/34004)\n",
            "epoch:2 batch:8600 loss:1.5493 acc:0.4487(15437/34404)\n",
            "epoch:2 batch:8700 loss:1.5493 acc:0.4488(15619/34804)\n",
            "epoch:2 batch:8800 loss:1.5492 acc:0.4484(15785/35204)\n",
            "epoch:2 batch:8900 loss:1.5487 acc:0.4486(15971/35604)\n",
            "epoch:2 batch:9000 loss:1.5492 acc:0.4482(16138/36004)\n",
            "epoch:2 batch:9100 loss:1.5494 acc:0.4482(16318/36404)\n",
            "epoch:2 batch:9200 loss:1.5493 acc:0.4484(16503/36804)\n",
            "epoch:2 batch:9300 loss:1.5502 acc:0.4482(16674/37204)\n",
            "epoch:2 batch:9400 loss:1.5499 acc:0.4484(16861/37604)\n",
            "epoch:2 batch:9500 loss:1.5493 acc:0.4484(17042/38004)\n",
            "epoch:2 batch:9600 loss:1.5502 acc:0.4482(17211/38404)\n",
            "epoch:2 batch:9700 loss:1.5496 acc:0.4483(17396/38804)\n",
            "epoch:2 batch:9800 loss:1.5483 acc:0.4488(17594/39204)\n",
            "epoch:2 batch:9900 loss:1.5482 acc:0.4493(17793/39604)\n",
            "epoch:2 batch:10000 loss:1.5481 acc:0.4494(17976/40004)\n",
            "epoch:2 batch:10100 loss:1.5481 acc:0.4496(18167/40404)\n",
            "epoch:2 batch:10200 loss:1.5476 acc:0.4498(18353/40804)\n",
            "epoch:2 batch:10300 loss:1.5478 acc:0.4499(18538/41204)\n",
            "epoch:2 batch:10400 loss:1.5484 acc:0.4497(18709/41604)\n",
            "epoch:2 batch:10500 loss:1.5480 acc:0.4501(18906/42004)\n",
            "epoch:2 batch:10600 loss:1.5478 acc:0.4501(19086/42404)\n",
            "epoch:2 batch:10700 loss:1.5478 acc:0.4495(19241/42804)\n",
            "epoch:2 batch:10800 loss:1.5481 acc:0.4495(19421/43204)\n",
            "epoch:2 batch:10900 loss:1.5481 acc:0.4495(19601/43604)\n",
            "epoch:2 batch:11000 loss:1.5480 acc:0.4491(19760/44004)\n",
            "epoch:2 batch:11100 loss:1.5482 acc:0.4490(19936/44404)\n",
            "epoch:2 batch:11200 loss:1.5486 acc:0.4491(20120/44804)\n",
            "epoch:2 batch:11300 loss:1.5487 acc:0.4492(20304/45204)\n",
            "epoch:2 batch:11400 loss:1.5480 acc:0.4492(20486/45604)\n",
            "epoch:2 batch:11500 loss:1.5477 acc:0.4494(20672/46004)\n",
            "epoch:2 batch:11600 loss:1.5469 acc:0.4496(20861/46404)\n",
            "epoch:2 batch:11700 loss:1.5469 acc:0.4493(21030/46804)\n",
            "epoch:2 batch:11800 loss:1.5465 acc:0.4494(21214/47204)\n",
            "epoch:2 batch:11900 loss:1.5458 acc:0.4496(21405/47604)\n",
            "epoch:2 batch:12000 loss:1.5465 acc:0.4495(21578/48004)\n",
            "epoch:2 batch:12100 loss:1.5471 acc:0.4492(21744/48404)\n",
            "epoch:2 batch:12200 loss:1.5470 acc:0.4491(21917/48804)\n",
            "epoch:2 batch:12300 loss:1.5461 acc:0.4493(22105/49204)\n",
            "epoch:2 batch:12400 loss:1.5458 acc:0.4495(22295/49604)\n",
            "epoch:3 batch:0 loss:1.6533 acc:0.2500(1/4)\n",
            "epoch:3 batch:100 loss:1.5340 acc:0.4406(178/404)\n",
            "epoch:3 batch:200 loss:1.5278 acc:0.4403(354/804)\n",
            "epoch:3 batch:300 loss:1.5114 acc:0.4518(544/1204)\n",
            "epoch:3 batch:400 loss:1.5062 acc:0.4520(725/1604)\n",
            "epoch:3 batch:500 loss:1.5005 acc:0.4546(911/2004)\n",
            "epoch:3 batch:600 loss:1.4925 acc:0.4597(1105/2404)\n",
            "epoch:3 batch:700 loss:1.4920 acc:0.4597(1289/2804)\n",
            "epoch:3 batch:800 loss:1.4814 acc:0.4622(1481/3204)\n",
            "epoch:3 batch:900 loss:1.4744 acc:0.4631(1669/3604)\n",
            "epoch:3 batch:1000 loss:1.4828 acc:0.4615(1848/4004)\n",
            "epoch:3 batch:1100 loss:1.4809 acc:0.4653(2049/4404)\n",
            "epoch:3 batch:1200 loss:1.4708 acc:0.4698(2257/4804)\n",
            "epoch:3 batch:1300 loss:1.4709 acc:0.4708(2450/5204)\n",
            "epoch:3 batch:1400 loss:1.4737 acc:0.4695(2631/5604)\n",
            "epoch:3 batch:1500 loss:1.4718 acc:0.4737(2844/6004)\n",
            "epoch:3 batch:1600 loss:1.4749 acc:0.4716(3020/6404)\n",
            "epoch:3 batch:1700 loss:1.4732 acc:0.4733(3220/6804)\n",
            "epoch:3 batch:1800 loss:1.4698 acc:0.4742(3416/7204)\n",
            "epoch:3 batch:1900 loss:1.4735 acc:0.4730(3597/7604)\n",
            "epoch:3 batch:2000 loss:1.4688 acc:0.4769(3817/8004)\n",
            "epoch:3 batch:2100 loss:1.4642 acc:0.4783(4020/8404)\n",
            "epoch:3 batch:2200 loss:1.4584 acc:0.4807(4232/8804)\n",
            "epoch:3 batch:2300 loss:1.4628 acc:0.4797(4415/9204)\n",
            "epoch:3 batch:2400 loss:1.4600 acc:0.4819(4628/9604)\n",
            "epoch:3 batch:2500 loss:1.4589 acc:0.4811(4813/10004)\n",
            "epoch:3 batch:2600 loss:1.4588 acc:0.4800(4994/10404)\n",
            "epoch:3 batch:2700 loss:1.4574 acc:0.4799(5185/10804)\n",
            "epoch:3 batch:2800 loss:1.4588 acc:0.4797(5374/11204)\n",
            "epoch:3 batch:2900 loss:1.4603 acc:0.4787(5555/11604)\n",
            "epoch:3 batch:3000 loss:1.4599 acc:0.4789(5749/12004)\n",
            "epoch:3 batch:3100 loss:1.4575 acc:0.4800(5954/12404)\n",
            "epoch:3 batch:3200 loss:1.4603 acc:0.4794(6138/12804)\n",
            "epoch:3 batch:3300 loss:1.4583 acc:0.4798(6335/13204)\n",
            "epoch:3 batch:3400 loss:1.4597 acc:0.4786(6511/13604)\n",
            "epoch:3 batch:3500 loss:1.4578 acc:0.4796(6717/14004)\n",
            "epoch:3 batch:3600 loss:1.4572 acc:0.4797(6910/14404)\n",
            "epoch:3 batch:3700 loss:1.4579 acc:0.4799(7105/14804)\n",
            "epoch:3 batch:3800 loss:1.4571 acc:0.4802(7301/15204)\n",
            "epoch:3 batch:3900 loss:1.4573 acc:0.4803(7494/15604)\n",
            "epoch:3 batch:4000 loss:1.4570 acc:0.4798(7678/16004)\n",
            "epoch:3 batch:4100 loss:1.4546 acc:0.4800(7874/16404)\n",
            "epoch:3 batch:4200 loss:1.4546 acc:0.4806(8076/16804)\n",
            "epoch:3 batch:4300 loss:1.4543 acc:0.4806(8268/17204)\n",
            "epoch:3 batch:4400 loss:1.4547 acc:0.4807(8462/17604)\n",
            "epoch:3 batch:4500 loss:1.4534 acc:0.4806(8653/18004)\n",
            "epoch:3 batch:4600 loss:1.4543 acc:0.4803(8839/18404)\n",
            "epoch:3 batch:4700 loss:1.4549 acc:0.4802(9030/18804)\n",
            "epoch:3 batch:4800 loss:1.4553 acc:0.4806(9229/19204)\n",
            "epoch:3 batch:4900 loss:1.4542 acc:0.4815(9439/19604)\n",
            "epoch:3 batch:5000 loss:1.4543 acc:0.4814(9630/20004)\n",
            "epoch:3 batch:5100 loss:1.4532 acc:0.4816(9826/20404)\n",
            "epoch:3 batch:5200 loss:1.4520 acc:0.4821(10029/20804)\n",
            "epoch:3 batch:5300 loss:1.4517 acc:0.4820(10220/21204)\n",
            "epoch:3 batch:5400 loss:1.4504 acc:0.4825(10425/21604)\n",
            "epoch:3 batch:5500 loss:1.4491 acc:0.4834(10637/22004)\n",
            "epoch:3 batch:5600 loss:1.4492 acc:0.4837(10837/22404)\n",
            "epoch:3 batch:5700 loss:1.4498 acc:0.4832(11018/22804)\n",
            "epoch:3 batch:5800 loss:1.4468 acc:0.4842(11236/23204)\n",
            "epoch:3 batch:5900 loss:1.4473 acc:0.4839(11422/23604)\n",
            "epoch:3 batch:6000 loss:1.4480 acc:0.4839(11615/24004)\n",
            "epoch:3 batch:6100 loss:1.4479 acc:0.4841(11813/24404)\n",
            "epoch:3 batch:6200 loss:1.4469 acc:0.4843(12012/24804)\n",
            "epoch:3 batch:6300 loss:1.4461 acc:0.4850(12225/25204)\n",
            "epoch:3 batch:6400 loss:1.4453 acc:0.4856(12433/25604)\n",
            "epoch:3 batch:6500 loss:1.4450 acc:0.4856(12628/26004)\n",
            "epoch:3 batch:6600 loss:1.4450 acc:0.4854(12816/26404)\n",
            "epoch:3 batch:6700 loss:1.4447 acc:0.4856(13016/26804)\n",
            "epoch:3 batch:6800 loss:1.4445 acc:0.4860(13220/27204)\n",
            "epoch:3 batch:6900 loss:1.4440 acc:0.4866(13431/27604)\n",
            "epoch:3 batch:7000 loss:1.4434 acc:0.4865(13623/28004)\n",
            "epoch:3 batch:7100 loss:1.4423 acc:0.4867(13823/28404)\n",
            "epoch:3 batch:7200 loss:1.4409 acc:0.4872(14033/28804)\n",
            "epoch:3 batch:7300 loss:1.4393 acc:0.4878(14246/29204)\n",
            "epoch:3 batch:7400 loss:1.4375 acc:0.4887(14466/29604)\n",
            "epoch:3 batch:7500 loss:1.4373 acc:0.4887(14663/30004)\n",
            "epoch:3 batch:7600 loss:1.4360 acc:0.4890(14867/30404)\n",
            "epoch:3 batch:7700 loss:1.4363 acc:0.4888(15056/30804)\n",
            "epoch:3 batch:7800 loss:1.4357 acc:0.4892(15266/31204)\n",
            "epoch:3 batch:7900 loss:1.4349 acc:0.4895(15470/31604)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-32b344736c69>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-611b00fe26fb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;31m# scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-611b00fe26fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}